{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJXW_DgiSebM"
      },
      "source": [
        "# LangGraph and LangSmith - Agentic RAG Powered by LangChain\n",
        "\n",
        "In the following notebook we'll complete the following tasks:\n",
        "\n",
        "- 🤝 Breakout Room #1:\n",
        "  1. Install required libraries\n",
        "  2. Set Environment Variables\n",
        "  3. Creating our Tool Belt\n",
        "  4. Creating Our State\n",
        "  5. Creating and Compiling A Graph!\n",
        "  \n",
        "\n",
        "- 🤝 Breakout Room #2:\n",
        "  1. Creating an Evaluation Dataset\n",
        "  2. Adding Evaluators\n",
        "  3. Evaluating"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djQ3nRAgoF67"
      },
      "source": [
        "# 🤝 Breakout Room #1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7pQDUhUnIo8"
      },
      "source": [
        "## LangGraph - Building Cyclic Applications with LangChain\n",
        "\n",
        "LangGraph is a tool that leverages LangChain Expression Language to build coordinated multi-actor and stateful applications that includes cyclic behaviour.\n",
        "\n",
        "### Why Cycles?\n",
        "\n",
        "In essence, we can think of a cycle in our graph as a more robust and customizable loop. It allows us to keep our application agent-forward while still giving the powerful functionality of traditional loops.\n",
        "\n",
        "Due to the inclusion of cycles over loops, we can also compose rather complex flows through our graph in a much more readable and natural fashion. Effetively allowing us to recreate appliation flowcharts in code in an almost 1-to-1 fashion.\n",
        "\n",
        "### Why LangGraph?\n",
        "\n",
        "Beyond the agent-forward approach - we can easily compose and combine traditional \"DAG\" (directed acyclic graph) chains with powerful cyclic behaviour due to the tight integration with LCEL. This means it's a natural extension to LangChain's core offerings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3_fLDElOVoop"
      },
      "source": [
        "## Task 1:  Dependencies\n",
        "\n",
        "We'll first install all our required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KaVwN269EttM",
        "outputId": "969dc3eb-14da-429a-f7be-230dcbafe749"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m810.5/810.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.1/269.1 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.4/262.4 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.1/81.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain langchain_openai langgraph arxiv duckduckgo-search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wujPjGJuoPwg"
      },
      "source": [
        "## Task 2: Environment Variables\n",
        "\n",
        "We'll want to set both our OpenAI API key and our LangSmith environment variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jdh8CoVWHRvs",
        "outputId": "d1484980-d381-4612-e3d2-bfe4786a931c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nv0glIDyHmRt",
        "outputId": "7718b9bc-296a-4864-c58f-20580343cf49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LangSmith API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE1 - LangGraph - {uuid4().hex[0:8]}\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangSmith API Key: \")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBRyQmEAVzua"
      },
      "source": [
        "## Task 3: Creating our Tool Belt\n",
        "\n",
        "As is usually the case, we'll want to equip our agent with a toolbelt to help answer questions and add external knowledge.\n",
        "\n",
        "There's a tonne of tools in the [LangChain Community Repo](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools) but we'll stick to a couple just so we can observe the cyclic nature of LangGraph in action!\n",
        "\n",
        "We'll leverage:\n",
        "\n",
        "- [Duck Duck Go Web Search](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/ddg_search)\n",
        "- [Arxiv](https://github.com/langchain-ai/langchain/tree/master/libs/community/langchain_community/tools/arxiv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2k6n_Dob2F46"
      },
      "source": [
        "####🏗️ Activity #1:\n",
        "\n",
        "Please add the tools to use into our toolbelt.\n",
        "\n",
        "> NOTE: Each tool in our toolbelt should be a method."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langchain_community.tools as langchain_tools\n",
        "\n",
        "type(langchain_tools)\n",
        "dir(langchain_tools)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XMWUYkUZR-tj",
        "outputId": "32fd63d4-1f90-49e0-c328-dacd2e3f7354"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Any',\n",
              " '_DEPRECATED_TOOLS',\n",
              " '__all__',\n",
              " '__builtins__',\n",
              " '__cached__',\n",
              " '__doc__',\n",
              " '__file__',\n",
              " '__getattr__',\n",
              " '__loader__',\n",
              " '__name__',\n",
              " '__package__',\n",
              " '__path__',\n",
              " '__spec__',\n",
              " '_module_lookup',\n",
              " 'importlib']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://python.langchain.com/docs/modules/agents/tools/\n",
        "def parse_tool(tool, query=None):\n",
        "  print(\"### Starting ...###\")\n",
        "  print(f\"Name : {tool.name}\")\n",
        "  print(f\"Description : {tool.description}\")\n",
        "  print(f\"Arg : {tool.args}\")\n",
        "  print(f\"Return direct : {tool.return_direct}\")\n",
        "  print(f\"Response of this quey = {query} \\n: {tool.run(query)}\")\n",
        "  print(\"### Ending ...###\\n\\n\")"
      ],
      "metadata": {
        "id": "r4RXbz1jT2vD"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/tools/ddg_search/tool.py\n",
        "\n",
        "- https://python.langchain.com/docs/modules/agents/tools/"
      ],
      "metadata": {
        "id": "FqDN2i7SXdp2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "\n",
        "tool = DuckDuckGoSearchRun()\n",
        "parse_tool(tool, \"Chicago\")\n",
        "\n",
        "\n",
        "tool = ArxivQueryRun()\n",
        "parse_tool(tool, \"Chicago\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMRfFEDwTf4s",
        "outputId": "b4059fd3-9857-45dc-ba9f-64b7b6ce7c01"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### Starting ...###\n",
            "Name : duckduckgo_search\n",
            "Description : A wrapper around DuckDuckGo Search. Useful for when you need to answer questions about current events. Input should be a search query.\n",
            "Arg : {'query': {'title': 'Query', 'description': 'search query to look up', 'type': 'string'}}\n",
            "Return direct : False\n",
            "Response of this quey = Chicago \n",
            ": The name Chicago is derived from a French rendering of the indigenous Miami-Illinois word shikaakwa for a wild relative of the onion; it is known to botanists as Allium tricoccum and known more commonly as \"ramps.\" The first known reference to the site of the current city of Chicago as \"Checagou\" was by Robert de LaSalle around 1679 in a memoir.Henri Joutel, in his journal of 1688, noted that ... Chicago's extensive Museum of Science and Industry is the Western Hemisphere's largest science museum, showcasing more than 35,000 artifacts and a variety of hands-on exhibits meant to inspire ... St. Regis Chicago is a new luxury entry in the city's hotel scene, with a buzzy restaurant and serene interiors. Perks include an indoor swimming pool with a view of the Chicago River. Rooms ... Chicago is a city, the seat of Cook county, in northeastern Illinois, U.S. With a population of nearly three million, Chicago is the state's largest and the country's third most populous city. It is the commercial and cultural hub of the American Midwest. Learn more about Chicago in this article. Discover the best things to do in Chicago, from museums and parks to jazz clubs and theaters. Explore the city's rich history, culture and nature with this curated list of the top attractions, from the Art Institute of Chicago to 360 CHICAGO.\n",
            "### Ending ...###\n",
            "\n",
            "\n",
            "### Starting ...###\n",
            "Name : arxiv\n",
            "Description : A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.\n",
            "Arg : {'query': {'title': 'Query', 'description': 'search query to look up', 'type': 'string'}}\n",
            "Return direct : False\n",
            "Response of this quey = Chicago \n",
            ": Published: 1995-10-13\n",
            "Title: Mechanisms for Stable Sonoluminescence\n",
            "Authors: Michael Brenner, Detlef Lohse, David Oxtoby, Todd Dupont\n",
            "Summary: A gas bubble trapped in water by an oscillating acoustic field is expected to\n",
            "either shrink or grow on a diffusive timescale, depending on the forcing\n",
            "strength and the bubble size. At high ambient gas concentration this has long\n",
            "been observed in experiments. However, recent sonoluminescence experiments show\n",
            "that in certain circumstances when the ambient gas concentration is low the\n",
            "bubble can be stable for days. This paper presents mechanisms leading to\n",
            "stability which predict parameter dependences in agreement with the\n",
            "sonoluminescence experiments.\n",
            "\n",
            "Published: 2020-03-24\n",
            "Title: Coronavirus Geographic Dissemination at Chicago and its Potential Proximity to Public Commuter Rail\n",
            "Authors: Peter Fang\n",
            "Summary: The community spread of coronavirus at great Chicago area has severely\n",
            "threatened the residents health, family and normal activities. CDC daily\n",
            "updates on infected cases on County level are not satisfying to address publics\n",
            "concern on virus spread. On March 20th, NBC5 published case information of 435\n",
            "coronavirus infections. The data is relative comprehensive and of high value\n",
            "for understanding on the virus spread patterns at Chicago. Data engineering,\n",
            "natural language processing and Google map technology are applied to organize\n",
            "the data and retrieve geographic information of the virus. The analysis shows\n",
            "community spread in Chicago areas has a potential proximity relation with\n",
            "public commuter rail. Residents nearby major public commuter rails need limit\n",
            "outdoor activities during the outbreak and even the post-peak time.\n",
            "\n",
            "Published: 2015-01-29\n",
            "Title: Patterns in Illinois Educational School Data\n",
            "Authors: Cacey S. Stevens, Michael P. Marder, Sidney R. Nagel\n",
            "Summary: We examine Illinois educational data from standardized exams and analyze\n",
            "primary factors affecting the achievement of public school students. We focus\n",
            "on the simplest possible models: representation of data through visualizations\n",
            "and regressions on single variables. Exam scores are shown to depend on school\n",
            "type, location, and poverty concentration. For most schools in Illinois,\n",
            "student test scores decline linearly with poverty concentration. However\n",
            "Chicago must be treated separately. Selective schools in Chicago, as well as\n",
            "some traditional and charter schools, deviate from this pattern based on\n",
            "poverty. For any poverty level, Chicago schools perform better than those in\n",
            "the rest of Illinois. Selective programs for gifted students show high\n",
            "performance at each grade level, most notably at the high school level, when\n",
            "compared to other Illinois school types. The case of Chicago charter schools is\n",
            "more complex. In the last six years, their students' scores overtook those of\n",
            "students in traditional Chicago high schools.\n",
            "### Ending ...###\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "lAxaSvlfIeOg"
      },
      "outputs": [],
      "source": [
        "from langchain_community.tools.ddg_search import DuckDuckGoSearchRun\n",
        "from langchain_community.tools.arxiv.tool import ArxivQueryRun\n",
        "\n",
        "tool_belt = [DuckDuckGoSearchRun(), ArxivQueryRun()\n",
        "    ### YOUR TOOL HERE,\n",
        "    ### YOUR TOOL HERE,\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FdOjEslXdRR"
      },
      "source": [
        "### Actioning with Tools\n",
        "\n",
        "Now that we've created our tool belt - we need to create a process that will let us leverage them when we need them.\n",
        "\n",
        "We'll use the built-in [`ToolExecutor`](https://github.com/langchain-ai/langgraph/blob/main/langgraph/prebuilt/tool_executor.py) to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cFr1m80-JZsD"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolExecutor\n",
        "\n",
        "tool_executor = ToolExecutor(tool_belt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VI-C669ZYVI5"
      },
      "source": [
        "### Model\n",
        "\n",
        "Now we can set-up our model! We'll leverage the familiar OpenAI model suite for this example - but it's not *necessary* to use with LangGraph. LangGraph supports all models - though you might not find success with smaller models - as such, they recommend you stick with:\n",
        "\n",
        "- OpenAI's GPT-3.5 and GPT-4\n",
        "- Anthropic's Claude\n",
        "- Google's Gemini\n",
        "\n",
        "> NOTE: Because we're leveraging the OpenAI function calling API - we'll need to use OpenAI *for this specific example* (or any other service that exposes an OpenAI-style function calling API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "QkNS8rNZJs4z"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(temperature=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ugkj3GzuZpQv"
      },
      "source": [
        "Now that we have our model set-up, let's \"put on the tool belt\", which is to say: We'll bind our LangChain formatted tools to the model in an OpenAI function calling format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4OdMqFafZ_0V"
      },
      "outputs": [],
      "source": [
        "from langchain_core.utils.function_calling import convert_to_openai_function\n",
        "\n",
        "functions = [convert_to_openai_function(t) for t in tool_belt]\n",
        "model = model.bind_functions(functions)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "id": "5ICcSIEEWMEM",
        "outputId": "3256a878-a7de-4ea6-f704-654d4c72a8f5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "langchain_core.runnables.base.RunnableBinding"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>langchain_core.runnables.base.RunnableBinding</b><br/>def __init__(*, bound: Runnable[Input, Output], kwargs: Optional[Mapping[str, Any]]=None, config: Optional[RunnableConfig]=None, config_factories: Optional[List[Callable[[RunnableConfig], RunnableConfig]]]=None, custom_input_type: Optional[Union[Type[Input], BaseModel]]=None, custom_output_type: Optional[Union[Type[Output], BaseModel]]=None, **other_kwargs: Any) -&gt; None</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py</a>Wrap a Runnable with additional functionality.\n",
              "\n",
              "A RunnableBinding can be thought of as a &quot;runnable decorator&quot; that\n",
              "preserves the essential features of Runnable; i.e., batching, streaming,\n",
              "and async support, while adding additional functionality.\n",
              "\n",
              "Any class that inherits from Runnable can be bound to a `RunnableBinding`.\n",
              "Runnables expose a standard set of methods for creating `RunnableBindings`\n",
              "or sub-classes of `RunnableBindings` (e.g., `RunnableRetry`,\n",
              "`RunnableWithFallbacks`) that add additional functionality.\n",
              "\n",
              "These methods include:\n",
              "- `bind`: Bind kwargs to pass to the underlying runnable when running it.\n",
              "- `with_config`: Bind config to pass to the underlying runnable when running it.\n",
              "- `with_listeners`:  Bind lifecycle listeners to the underlying runnable.\n",
              "- `with_types`: Override the input and output types of the underlying runnable.\n",
              "- `with_retry`: Bind a retry policy to the underlying runnable.\n",
              "- `with_fallbacks`: Bind a fallback policy to the underlying runnable.\n",
              "\n",
              "Example:\n",
              "\n",
              "`bind`: Bind kwargs to pass to the underlying runnable when running it.\n",
              "\n",
              "    .. code-block:: python\n",
              "\n",
              "        # Create a runnable binding that invokes the ChatModel with the\n",
              "        # additional kwarg `stop=[&#x27;-&#x27;]` when running it.\n",
              "        from langchain_community.chat_models import ChatOpenAI\n",
              "        model = ChatOpenAI()\n",
              "        model.invoke(&#x27;Say &quot;Parrot-MAGIC&quot;&#x27;, stop=[&#x27;-&#x27;]) # Should return `Parrot`\n",
              "        # Using it the easy way via `bind` method which returns a new\n",
              "        # RunnableBinding\n",
              "        runnable_binding = model.bind(stop=[&#x27;-&#x27;])\n",
              "        runnable_binding.invoke(&#x27;Say &quot;Parrot-MAGIC&quot;&#x27;) # Should return `Parrot`\n",
              "\n",
              "    Can also be done by instantiating a RunnableBinding directly (not recommended):\n",
              "\n",
              "    .. code-block:: python\n",
              "\n",
              "        from langchain_core.runnables import RunnableBinding\n",
              "        runnable_binding = RunnableBinding(\n",
              "            bound=model,\n",
              "            kwargs={&#x27;stop&#x27;: [&#x27;-&#x27;]} # &lt;-- Note the additional kwargs\n",
              "        )\n",
              "        runnable_binding.invoke(&#x27;Say &quot;Parrot-MAGIC&quot;&#x27;) # Should return `Parrot`</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 4555);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ERzuGo6W18Lr"
      },
      "source": [
        "#### ❓ Question #1:\n",
        "\n",
        "How does the model determine which tool to use?\n",
        "\n",
        "#### Answer #1:\n",
        "A description of what the tool is"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_296Ub96Z_H8"
      },
      "source": [
        "## Putting the State in Stateful\n",
        "\n",
        "Earlier we used this phrasing:\n",
        "\n",
        "`coordinated multi-actor and stateful applications`\n",
        "\n",
        "So what does that \"stateful\" mean?\n",
        "\n",
        "To put it simply - we want to have some kind of object which we can pass around our application that holds information about what the current situation (state) is. Since our system will be constructed of many parts moving in a coordinated fashion - we want to be able to ensure we have some commonly understood idea of that state.\n",
        "\n",
        "LangGraph leverages a `StatefulGraph` which uses an `AgentState` object to pass information between the various nodes of the graph.\n",
        "\n",
        "There are more options than what we'll see below - but this `AgentState` object is one that is stored in a `TypedDict` with the key `messages` and the value is a `Sequence` of `BaseMessages` that will be appended to whenever the state changes.\n",
        "\n",
        "Let's think about a simple example to help understand exactly what this means (we'll simplify a great deal to try and clearly communicate what state is doing):\n",
        "\n",
        "1. We initialize our state object:\n",
        "  - `{\"messages\" : []}`\n",
        "2. Our user submits a query to our application.\n",
        "  - New State: `HumanMessage(#1)`\n",
        "  - `{\"messages\" : [HumanMessage(#1)}`\n",
        "3. We pass our state object to an Agent node which is able to read the current state. It will use the last `HumanMessage` as input. It gets some kind of output which it will add to the state.\n",
        "  - New State: `AgentMessage(#1, additional_kwargs {\"function_call\" : \"WebSearchTool\"})`\n",
        "  - `{\"messages\" : [HumanMessage(#1), AgentMessage(#1, ...)]}`\n",
        "4. We pass our state object to a \"conditional node\" (more on this later) which reads the last state to determine if we need to use a tool - which it can determine properly because of our provided object!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mxL9b_NZKUdL"
      },
      "outputs": [],
      "source": [
        "from typing import TypedDict, Annotated, Sequence\n",
        "import operator\n",
        "from langchain_core.messages import BaseMessage\n",
        "\n",
        "class AgentState(TypedDict):\n",
        "  messages: Annotated[Sequence[BaseMessage], operator.add]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWsMhfO9grLu"
      },
      "source": [
        "## It's Graphing Time!\n",
        "\n",
        "Now that we have state, and we have tools, and we have an LLM - we can finally start making our graph!\n",
        "\n",
        "Let's take a second to refresh ourselves about what a graph is in this context.\n",
        "\n",
        "Graphs, also called networks in some circles, are a collection of connected objects.\n",
        "\n",
        "The objects in question are typically called nodes, or vertices, and the connections are called edges.\n",
        "\n",
        "Let's look at a simple graph.\n",
        "\n",
        "![image](https://i.imgur.com/2NFLnIc.png)\n",
        "\n",
        "Here, we're using the coloured circles to represent the nodes and the yellow lines to represent the edges. In this case, we're looking at a fully connected graph - where each node is connected by an edge to each other node.\n",
        "\n",
        "If we were to think about nodes in the context of LangGraph - we would think of a function, or an LCEL runnable.\n",
        "\n",
        "If we were to think about edges in the context of LangGraph - we might think of them as \"paths to take\" or \"where to pass our state object next\".\n",
        "\n",
        "Let's create some nodes and expand on our diagram.\n",
        "\n",
        "> NOTE: Due to the tight integration with LCEL - we can comfortably create our nodes in an async fashion!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "91flJWtZLUrl"
      },
      "outputs": [],
      "source": [
        "from langgraph.prebuilt import ToolInvocation\n",
        "import json\n",
        "from langchain_core.messages import FunctionMessage\n",
        "\n",
        "def call_model(state):\n",
        "  messages = state[\"messages\"]\n",
        "  response = model.invoke(messages)\n",
        "  return {\"messages\" : [response]}\n",
        "\n",
        "def call_tool(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  action = ToolInvocation(\n",
        "      tool=last_message.additional_kwargs[\"function_call\"][\"name\"],\n",
        "      tool_input=json.loads(\n",
        "          last_message.additional_kwargs[\"function_call\"][\"arguments\"]\n",
        "      )\n",
        "  )\n",
        "\n",
        "  response = tool_executor.invoke(action)\n",
        "\n",
        "  function_message = FunctionMessage(content=str(response), name=action.tool)\n",
        "\n",
        "  return {\"messages\" : [function_message]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bwR7MgWj3Wg"
      },
      "source": [
        "Now we have two total nodes. We have:\n",
        "\n",
        "- `call_model` is a node that will...well...call the model\n",
        "- `call_tool` is a node which will call a tool\n",
        "\n",
        "Let's start adding nodes! We'll update our diagram along the way to keep track of what this looks like!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "_vF4_lgtmQNo"
      },
      "outputs": [],
      "source": [
        "from langgraph.graph import StateGraph, END\n",
        "\n",
        "workflow = StateGraph(AgentState)\n",
        "\n",
        "workflow.add_node(\"agent\", call_model)\n",
        "workflow.add_node(\"action\", call_tool)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8CjRlbVmRpW"
      },
      "source": [
        "Let's look at what we have so far:\n",
        "\n",
        "![image](https://i.imgur.com/md7inqG.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uaXHpPeSnOWC"
      },
      "source": [
        "Next, we'll add our entrypoint. All our entrypoint does is indicate which node is called first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "YGCbaYqRnmiw"
      },
      "outputs": [],
      "source": [
        "workflow.set_entry_point(\"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUsfGoSpoF9U"
      },
      "source": [
        "![image](https://i.imgur.com/wNixpJe.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Q_pQgHmoW0M"
      },
      "source": [
        "Now we want to build a \"conditional edge\" which will use the output state of a node to determine which path to follow.\n",
        "\n",
        "We can help conceptualize this by thinking of our conditional edge as a conditional in a flowchart!\n",
        "\n",
        "Notice how our function simply checks if there is a \"function_call\" kwarg present.\n",
        "\n",
        "Then we create an edge where the origin node is our agent node and our destination node is *either* the action node or the END (finish the graph).\n",
        "\n",
        "It's important to highlight that the dictionary passed in as the third parameter (the mapping) should be created with the possible outputs of our conditional function in mind. In this case `should_continue` outputs either `\"end\"` or `\"continue\"` which are subsequently mapped to the action node or the END node."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "1BZgb81VQf9o"
      },
      "outputs": [],
      "source": [
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  if \"function_call\" not in last_message.additional_kwargs:\n",
        "    return \"end\"\n",
        "\n",
        "  return \"continue\"\n",
        "\n",
        "workflow.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    should_continue,\n",
        "    {\n",
        "        \"continue\" : \"action\",\n",
        "        \"end\" : END\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Cvhcf4jp0Ce"
      },
      "source": [
        "Let's visualize what this looks like.\n",
        "\n",
        "![image](https://i.imgur.com/8ZNwKI5.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKCjWJCkrJb9"
      },
      "source": [
        "Finally, we can add our last edge which will connect our action node to our agent node. This is because we *always* want our action node (which is used to call our tools) to return its output to our agent!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UvcgbHf1rIXZ"
      },
      "outputs": [],
      "source": [
        "workflow.add_edge(\"action\", \"agent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiWDwBQtrw7Z"
      },
      "source": [
        "Let's look at the final visualization.\n",
        "\n",
        "![image](https://i.imgur.com/NWO7usO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYqDpErlsCsu"
      },
      "source": [
        "All that's left to do now is to compile our workflow - and we're off!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "zt9-KS8DpzNx"
      },
      "outputs": [],
      "source": [
        "app = workflow.compile()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhNWIwBL1W4Q"
      },
      "source": [
        "#### ❓ Question #2:\n",
        "\n",
        "Is there any specific limit to how many times we can cycle?\n",
        "\n",
        "If not, how could we impose a limit to the number of cycles?\n",
        "\n",
        "\n",
        "#### Answer #2:\n",
        "\n",
        "- There is no explicit limit to the number of cycles (iterations) within the application.\n",
        "\n",
        "- The workflow cycles between the \"agent\" and \"action\" nodes based on the `should_continue` function's decision. So we limit the number of cycles by adding `MAX_CYCLE` in the conditional function\n",
        "\n",
        "\n",
        "```\n",
        "MAX_CYCLE = 100\n",
        "def should_continue(state):\n",
        "  last_message = state[\"messages\"][-1]\n",
        "\n",
        "  state[\"cycle\"] += 1\n",
        "\n",
        "  if (\"function_call\" not in last_message.additional_kwargs) or (state[\"cycle\"]>=MAX_CYCLE):\n",
        "    return \"end\"\n",
        "\n",
        "  return \"continue\"\n",
        "```\n",
        "\n",
        "https://medium.com/data-science-in-your-pocket/improving-rag-using-langgraph-and-langchain-bb195bfe4b44\n",
        "\n",
        "\n",
        "https://blog.langchain.dev/reflection-agents/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEYcTShCsPaa"
      },
      "source": [
        "## Using Our Graph\n",
        "\n",
        "Now that we've created and compiled our graph - we can call it *just as we'd call any other* `Runnable`!\n",
        "\n",
        "Let's try out a few examples to see how it fairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qn4n37PQRPII",
        "outputId": "e24b6179-c0da-44dc-e6eb-e7ee2bcc05d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='What is RAG in the context of Large Language Models? When did it break onto the scene?'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"RAG in the context of Large Language Models\"}', 'name': 'duckduckgo_search'}}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 171, 'total_tokens': 196}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_4f0b692a78', 'finish_reason': 'function_call', 'logprobs': None}),\n",
              "  FunctionMessage(content='Retrieval-augmented generation (RAG) is a technique used to \"ground\" large language models (LLMs) with specific data sources, often sources that weren\\'t included in the models\\' original ... The beauty of RAG lies in its ability to enable a language model to draw upon and leverage your own data to generate responses. While base models are traditionally trained on specific, point-in-time data, ensuring their effectiveness in performing tasks and adapting to the desired domain, they can struggle when faced with newer or current data. Dynamic retrieval augmented generation (RAG) paradigm actively decides when and what to retrieve during the text generation process of Large Language Models (LLMs). There are two key elements of this paradigm: identifying the optimal moment to activate the retrieval module (deciding when to retrieve) and crafting the appropriate query once retrieval is triggered (determining what to retrieve ... Key Takeaways. RAG is a relatively new artificial intelligence technique that can improve the quality of generative AI by allowing large language model (LLMs) to tap additional data resources without retraining. RAG models build knowledge repositories based on the organization\\'s own data, and the repositories can be continually updated to ... The Role of RAG in LLMs. RAG (Retrieval-Augmented Generation) is a pivotal technique in LLMs (Large Language Models), designed to mitigate errors or \\'hallucinations.\\'. It achieves this by basing model responses on context drawn from external data sources. This approach significantly enhances cost-efficiency in LLM operations.', name='duckduckgo_search'),\n",
              "  AIMessage(content=\"Retrieval-augmented generation (RAG) is a technique used in the context of Large Language Models (LLMs) to improve the quality of generative AI by allowing LLMs to tap into additional data resources without the need for retraining. RAG models build knowledge repositories based on the organization's own data, which can be continually updated. RAG helps LLMs generate responses based on context drawn from external data sources, enhancing cost-efficiency in LLM operations.\\n\\nRAG broke onto the scene as a relatively new artificial intelligence technique that can enhance the capabilities of LLMs by enabling them to leverage additional data sources for generating responses. It is designed to mitigate errors or 'hallucinations' in LLMs by incorporating external data into the generation process.\", response_metadata={'token_usage': {'completion_tokens': 158, 'prompt_tokens': 518, 'total_tokens': 676}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_4f0b692a78', 'finish_reason': 'stop', 'logprobs': None})]}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "inputs = {\"messages\" : [HumanMessage(content=\"What is RAG in the context of Large Language Models? When did it break onto the scene?\")]}\n",
        "\n",
        "app.invoke(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBHnUtLSscRr"
      },
      "source": [
        "Let's look at what happened:\n",
        "\n",
        "1. Our state object was populated with our request\n",
        "2. The state object was passed into our entry point (agent node) and the agent node added an `AIMessage` to the state object and passed it along the conditional edge\n",
        "3. The conditional edge received the state object, found the \"function_call\" `additional_kwarg`, and sent the state object to the action node\n",
        "4. The action node added the response from the OpenAI function calling endpoint to the state object and passed it along the edge to the agent node\n",
        "5. The agent node added a response to the state object and passed it along the conditional edge\n",
        "6. The conditional edge received the state object, could not find the \"function_call\" `additional_kwarg` and passed the state object to END where we see it output in the cell above!\n",
        "\n",
        "Now let's look at an example that shows a multiple tool usage - all with the same flow!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afv2BuEsV5JG",
        "outputId": "2351b1b5-5f78-4aad-dd22-274a4246704c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'messages': [HumanMessage(content='What is QLoRA in Machine Learning? Are their any papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"QLoRA in Machine Learning\"}', 'name': 'arxiv'}}, response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 193, 'total_tokens': 212}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_fa89f7a861', 'finish_reason': 'function_call', 'logprobs': None}),\n",
              "  FunctionMessage(content=\"Published: 2023-05-23\\nTitle: QLoRA: Efficient Finetuning of Quantized LLMs\\nAuthors: Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, Luke Zettlemoyer\\nSummary: We present QLoRA, an efficient finetuning approach that reduces memory usage\\nenough to finetune a 65B parameter model on a single 48GB GPU while preserving\\nfull 16-bit finetuning task performance. QLoRA backpropagates gradients through\\na frozen, 4-bit quantized pretrained language model into Low Rank\\nAdapters~(LoRA). Our best model family, which we name Guanaco, outperforms all\\nprevious openly released models on the Vicuna benchmark, reaching 99.3% of the\\nperformance level of ChatGPT while only requiring 24 hours of finetuning on a\\nsingle GPU. QLoRA introduces a number of innovations to save memory without\\nsacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is\\ninformation theoretically optimal for normally distributed weights (b) double\\nquantization to reduce the average memory footprint by quantizing the\\nquantization constants, and (c) paged optimziers to manage memory spikes. We\\nuse QLoRA to finetune more than 1,000 models, providing a detailed analysis of\\ninstruction following and chatbot performance across 8 instruction datasets,\\nmultiple model types (LLaMA, T5), and model scales that would be infeasible to\\nrun with regular finetuning (e.g. 33B and 65B parameter models). Our results\\nshow that QLoRA finetuning on a small high-quality dataset leads to\\nstate-of-the-art results, even when using smaller models than the previous\\nSoTA. We provide a detailed analysis of chatbot performance based on both human\\nand GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable\\nalternative to human evaluation. Furthermore, we find that current chatbot\\nbenchmarks are not trustworthy to accurately evaluate the performance levels of\\nchatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to\\nChatGPT. We release all of our models and code, including CUDA kernels for\\n4-bit training.\\n\\nPublished: 2023-12-31\\nTitle: Viz: A QLoRA-based Copyright Marketplace for Legally Compliant Generative AI\\nAuthors: Dipankar Sarkar\\nSummary: This paper aims to introduce and analyze the Viz system in a comprehensive\\nway, a novel system architecture that integrates Quantized Low-Rank Adapters\\n(QLoRA) to fine-tune large language models (LLM) within a legally compliant and\\nresource efficient marketplace. Viz represents a significant contribution to\\nthe field of artificial intelligence, particularly in addressing the challenges\\nof computational efficiency, legal compliance, and economic sustainability in\\nthe utilization and monetization of LLMs. The paper delineates the scholarly\\ndiscourse and developments that have informed the creation of Viz, focusing\\nprimarily on the advancements in LLM models, copyright issues in AI training\\n(NYT case, 2023), and the evolution of model fine-tuning techniques,\\nparticularly low-rank adapters and quantized low-rank adapters, to create a\\nsustainable and economically compliant framework for LLM utilization. The\\neconomic model it proposes benefits content creators, AI developers, and\\nend-users, delineating a harmonious integration of technology, economy, and\\nlaw, offering a comprehensive solution to the complex challenges of today's AI\\nlandscape.\\n\\nPublished: 2024-02-08\\nTitle: Accurate LoRA-Finetuning Quantization of LLMs via Information Retention\\nAuthors: Haotong Qin, Xudong Ma, Xingyu Zheng, Xiaoyang Li, Yang Zhang, Shouda Liu, Jie Luo, Xianglong Liu, Michele Magno\\nSummary: The LoRA-finetuning quantization of LLMs has been extensively studied to\\nobtain accurate yet compact LLMs for deployment on resource-constrained\\nhardware. However, existing methods cause the quantized LLM to severely degrade\\nand even fail to benefit from the finetuning of LoRA. This paper proposes a\\nnovel IR-QLoRA for pushing quantized LLMs with LoRA to be highly accurate\\nthrough information retention. The proposed IR-QLoRA mainly relies on two\\ntechnolog\", name='arxiv'),\n",
              "  AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"query\":\"Tim Dettmers bio\"}', 'name': 'duckduckgo_search'}}, response_metadata={'token_usage': {'completion_tokens': 21, 'prompt_tokens': 1162, 'total_tokens': 1183}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_4f0b692a78', 'finish_reason': 'function_call', 'logprobs': None}),\n",
              "  FunctionMessage(content=\"Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ... Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. Bio: Tim Dettmers's research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. This involves developing novel compression and networking algorithms and building systems that allow for memory-efficient, fast, and cheap deep learning. ...\", name='duckduckgo_search'),\n",
              "  AIMessage(content='QLoRA (Quantized Low-Rank Adapters) is an efficient finetuning approach that reduces memory usage while preserving performance when finetuning large language models (LLMs) in machine learning. It involves backpropagating gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters (LoRA). The paper \"QLoRA: Efficient Finetuning of Quantized LLMs\" by Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer presents this approach, showcasing its effectiveness in finetuning models on a single GPU with reduced memory requirements.\\n\\nThe first author of the QLoRA paper is Tim Dettmers. Tim Dettmers\\'s research focuses on making foundation models, such as ChatGPT, accessible to researchers and practitioners by reducing their resource requirements. He works on developing novel compression and networking algorithms and building systems that enable memory-efficient, fast, and cost-effective deep learning solutions.', response_metadata={'token_usage': {'completion_tokens': 202, 'prompt_tokens': 1472, 'total_tokens': 1674}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': 'fp_fa89f7a861', 'finish_reason': 'stop', 'logprobs': None})]}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "inputs = {\"messages\" : [HumanMessage(content=\"What is QLoRA in Machine Learning? Are their any papers that could help me understand? Once you have that information, can you look up the bio of the first author on the QLoRA paper?\")]}\n",
        "\n",
        "app.invoke(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CXzDlZVz1Hnf"
      },
      "source": [
        "####🏗️ Activity #2:\n",
        "\n",
        "Please write out the steps the agent took to arrive at the correct answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV3XeFOT1Sar"
      },
      "source": [
        "### Pre-processing for LangSmith"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wruQCuzewUuO"
      },
      "source": [
        "To do a little bit more preprocessing, let's wrap our LangGraph agent in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "oeXdQgbxwhTv"
      },
      "outputs": [],
      "source": [
        "def convert_inputs(input_object):\n",
        "  return {\"messages\" : [HumanMessage(content=input_object[\"question\"])]}\n",
        "\n",
        "def parse_output(input_state):\n",
        "  return input_state[\"messages\"][-1].content\n",
        "\n",
        "agent_chain = convert_inputs | app | parse_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "id": "orYxBZXSxJjZ",
        "outputId": "b2b7104a-e997-4382-f6a7-6a642ed24f38"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"RAG stands for Retrieval-Augmented Generation. It is an AI framework for improving the quality of language model-generated responses by grounding the model on external sources of knowledge to supplement the model's internal representation of information. RAG enhances the accuracy and reliability of generative AI models by fetching facts from external sources. It uses a combination of parametric memory represented by a pre-trained seq2seq model and non-parametric memory in the form of a dense vector index of sources like Wikipedia. RAG models leverage both forms of memory to generate text.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "agent_chain.invoke({\"question\" : \"What is RAG?\"})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQmrzYfrm1Dr"
      },
      "source": [
        "# 🤝 Breakout Room #2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9UkCIqkpyZu"
      },
      "source": [
        "## Task 1: Creating An Evaluation Dataset\n",
        "\n",
        "Just as we saw last week, we'll want to create a dataset to test our Agent's ability to answer questions.\n",
        "\n",
        "In order to do this - we'll want to provide some questions and some answers. Let's look at how we can create such a dataset below.\n",
        "\n",
        "```python\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "    {\"must_mention\" : [\"paged\", \"optimizer\"]},\n",
        "    {\"must_mention\" : [\"NF4\", \"NormalFloat\"]},\n",
        "    {\"must_mention\" : [\"ground\", \"context\"]},\n",
        "    {\"must_mention\" : [\"Tim\", \"Dettmers\"]},\n",
        "    {\"must_mention\" : [\"PyTorch\", \"TensorFlow\"]},\n",
        "    {\"must_mention\" : [\"reduce\", \"parameters\"]},\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfMXF2KAsQxs"
      },
      "source": [
        "####🏗️ Activity #3:\n",
        "\n",
        "Please create a dataset in the above format with at least 5 questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "CbagRuJop83E"
      },
      "outputs": [],
      "source": [
        "# questions = [\n",
        "#     ### YOUR QUESTIONS HERE\n",
        "# ]\n",
        "\n",
        "# answers = [\n",
        "#     ### YOUR ANSWERS HERE\n",
        "# ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "questions = [\n",
        "\"What challenge does QLoRA address in natural language processing?\",\n",
        "\"How does the NF4 data type differ from traditional floating-point formats?\",\n",
        "\"What impact does Retrieval Augmented Generation have on chatbot development?\",\n",
        "\"Can QLoRA be applied to image recognition tasks?\",\n",
        "\"What advancements in optimization algorithms are proposed in the QLoRA paper?\"\n",
        "]\n",
        "\n",
        "answers = [\n",
        "{\"must_mention\" : [\"complexity\", \"large models\"]},\n",
        "{\"must_mention\" : [\"precision\", \"storage\"]},\n",
        "{\"must_mention\" : [\"relevance\", \"accuracy\"]},\n",
        "{\"must_mention\" : [\"yes\", \"adaptability\"]},\n",
        "{\"must_mention\" : [\"efficiency\", \"training speed\"]}\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7QVFuAmsh7L"
      },
      "source": [
        "Now we can add our dataset to our LangSmith project using the following code which we saw last Thursday!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "RLfrZrgSsn85"
      },
      "outputs": [],
      "source": [
        "from langsmith import Client\n",
        "\n",
        "client = Client()\n",
        "dataset_name = f\"Retrieval Augmented Generation - Evaluation Dataset - {uuid4().hex[0:8]}\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    outputs=answers,\n",
        "    dataset_id=dataset.id,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciV73F9Q04w0"
      },
      "source": [
        "#### ❓ Question #3:\n",
        "\n",
        "How are the correct answers associated with the questions?\n",
        "\n",
        "#### Answer #3:\n",
        "\n",
        "the answers is generated with high helpfulness and contain the almost all must_mention words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-lRTXUrTtP9Y"
      },
      "source": [
        "## Task 2: Adding Evaluators\n",
        "\n",
        "Now we can add a custom evaluator to see if our responses contain the expected information.\n",
        "\n",
        "We'll be using a fairly naive exact-match process to determine if our response contains specific strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "QrAUXMFftlAY"
      },
      "outputs": [],
      "source": [
        "from langsmith.evaluation import EvaluationResult, run_evaluator\n",
        "\n",
        "@run_evaluator\n",
        "def must_mention(run, example) -> EvaluationResult:\n",
        "    prediction = run.outputs.get(\"output\") or \"\"\n",
        "    required = example.outputs.get(\"must_mention\") or []\n",
        "    score = all(phrase in prediction for phrase in required)\n",
        "    return EvaluationResult(key=\"must_mention\", score=score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNtHORUh0jZY"
      },
      "source": [
        "#### ❓ Question #4:\n",
        "\n",
        "What are some ways you could improve this metric as-is?\n",
        "\n",
        "> NOTE: Alternatively you can suggest where gaps exist in this method.\n",
        "\n",
        "\n",
        "\n",
        "#### Answer #4:\n",
        "\n",
        "- We can use paraphrase or synonyms to enhance the metric\n",
        "- Another way is to do semantic analysis and check if the predicted text semantically covers the topics or concepts of the must_mention phrases, even if it doesn't include the exact words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ4DVSXl0BX5"
      },
      "source": [
        "Now that we have created our custom evaluator - let's initialize our `RunEvalConfig` with it, and a few others:\n",
        "\n",
        "- `\"criteria\"` includes the default criteria which, in this case, means \"helpfulness\"\n",
        "- `\"cot_qa\"` includes a criteria that bases whether or not the answer is correct by utilizing a Chain of Thought prompt and the provided context to determine if the response is correct or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "sL4-XcjytWsu"
      },
      "outputs": [],
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[must_mention],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        \"cot_qa\",\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1RJr349zhv7"
      },
      "source": [
        "Task 3: Evaluating\n",
        "\n",
        "All that is left to do is evaluate our agent's response!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5TeCUUkuGld",
        "outputId": "1f19a296-07cd-41b4-fe59-6c39fd4345de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for project 'RAG Pipeline - Evaluation - 82ccda42' at:\n",
            "https://smith.langchain.com/o/ec0ab543-3d20-5b47-b548-e29646706b91/datasets/5ccabf42-7e56-472e-ac7a-be0b030db64f/compare?selectedSessions=4c48457d-75b3-4154-9c44-7358a48a763e\n",
            "\n",
            "View all tests for Dataset Retrieval Augmented Generation - Evaluation Dataset - 563bea22 at:\n",
            "https://smith.langchain.com/o/ec0ab543-3d20-5b47-b548-e29646706b91/datasets/5ccabf42-7e56-472e-ac7a-be0b030db64f\n",
            "[------------------------------------------------->] 5/5\n",
            " Experiment Results:\n",
            "        feedback.helpfulness  feedback.COT Contextual Accuracy feedback.must_mention error  execution_time                                run_id\n",
            "count                   5.00                              4.00                     5     0            5.00                                     5\n",
            "unique                   NaN                               NaN                     1     0             NaN                                     5\n",
            "top                      NaN                               NaN                 False   NaN             NaN  bc86fc6f-1ef4-4b44-b295-7d010cfbf121\n",
            "freq                     NaN                               NaN                     5   NaN             NaN                                     1\n",
            "mean                    1.00                              1.00                   NaN   NaN            4.99                                   NaN\n",
            "std                     0.00                              0.00                   NaN   NaN            1.09                                   NaN\n",
            "min                     1.00                              1.00                   NaN   NaN            3.30                                   NaN\n",
            "25%                     1.00                              1.00                   NaN   NaN            4.92                                   NaN\n",
            "50%                     1.00                              1.00                   NaN   NaN            5.10                                   NaN\n",
            "75%                     1.00                              1.00                   NaN   NaN            5.32                                   NaN\n",
            "max                     1.00                              1.00                   NaN   NaN            6.33                                   NaN\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'project_name': 'RAG Pipeline - Evaluation - 82ccda42',\n",
              " 'results': {'c9e1f6cb-7ed6-4303-a8e0-2fe8c7432477': {'input': {'question': 'What challenge does QLoRA address in natural language processing?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission should be evaluated based on whether it provides a helpful, insightful, and appropriate response to the input question. \\n\\nThe input question asks about the challenge that QLoRA addresses in natural language processing. \\n\\nThe submitted answer provides a detailed explanation of how QLoRA addresses a specific challenge in natural language processing, which is reducing memory usage during fine-tuning. \\n\\nThe answer is helpful because it provides a clear and direct response to the question. \\n\\nThe answer is insightful because it explains the specific methods that QLoRA uses to address the challenge, such as using a special data type and quantizing the weights of the LoRA adapters. \\n\\nThe answer is appropriate because it stays on topic and directly addresses the question. \\n\\nTherefore, the submission meets the criterion of being helpful, insightful, and appropriate. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('cebcfda4-b690-4515-badc-53a96baf6c50'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=None, value=None, comment=\"The context provided does not give specific information about QLoRA addressing the challenge of reducing memory usage during fine-tuning in natural language processing. It only mentions 'complexity' and 'large models' which could be related to many different challenges in natural language processing. The student's answer is detailed and specific about how QLoRA addresses a particular challenge, but this information cannot be confirmed or denied based on the provided context. Therefore, we cannot definitively say whether the student's answer is correct or incorrect based on the context given.\\nGRADE: INCONCLUSIVE\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('cd926f7f-adce-4264-9c1f-7049d9a03a62'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 3.300943,\n",
              "   'run_id': 'bc86fc6f-1ef4-4b44-b295-7d010cfbf121',\n",
              "   'output': 'QLoRA addresses the challenge of reducing memory usage during fine-tuning in natural language processing. It uses a special data type called 4-bit NormalFloat to encode numbers with just 4 bits, which helps in reducing memory usage while maintaining performance. QLoRA also quantizes the weights of the LoRA adapters to lower precision, further reducing memory requirements during the fine-tuning process.',\n",
              "   'reference': {'must_mention': ['complexity', 'large models']}},\n",
              "  '26f60906-af8a-4652-b88f-ad4925535ebe': {'input': {'question': 'How does the NF4 data type differ from traditional floating-point formats?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". The submission should be helpful, insightful, and appropriate.\\n\\nLooking at the submission, it provides a detailed explanation of how the NF4 data type differs from traditional floating-point formats. It explains the concept of NF4 data type, its precision, efficiency, range, and applications. \\n\\nThe submission is helpful as it provides a clear and comprehensive answer to the question. It is insightful as it not only explains the differences but also provides the reasons behind these differences. It is appropriate as it directly answers the question and stays on topic.\\n\\nTherefore, the submission meets the criterion.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('282f10ec-f1a4-4a10-a358-cd720772e901'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer is in line with the context provided. The student correctly identifies that the NF4 data type differs from traditional floating-point formats in terms of precision and storage efficiency. The student also correctly explains that the NF4 data type uses fewer bits to represent the significand, resulting in lower precision but faster computation and reduced memory usage. The student further explains that the range of values that can be represented by the NF4 data type is limited compared to traditional floating-point formats, and that the NF4 data type is particularly useful in applications where speed and memory efficiency are critical. Therefore, the student's answer is factually accurate.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('80c6daf5-2856-426b-8d0e-f5791ae5c446'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 5.097724,\n",
              "   'run_id': '2a26d74d-80e7-44a5-b59f-7215d3592b4d',\n",
              "   'output': 'The NF4 data type, also known as Near-Float 4, is a new data type designed to provide a balance between precision and efficiency in numerical computations. It differs from traditional floating-point formats in the following ways:\\n\\n1. Precision: NF4 data type offers reduced precision compared to traditional floating-point formats like single precision (32-bit) or double precision (64-bit). NF4 uses only 4 bits to represent the significand, resulting in lower precision but faster computation.\\n\\n2. Efficiency: NF4 data type is designed for efficient computation on modern hardware, especially for tasks that do not require high precision. By using a smaller data type, NF4 can reduce memory usage and improve computational efficiency.\\n\\n3. Range: The range of values that can be represented by NF4 data type is limited compared to traditional floating-point formats. NF4 sacrifices range for efficiency by using a smaller number of bits to represent the significand.\\n\\n4. Applications: NF4 data type is particularly useful in applications where speed and memory efficiency are critical, such as machine learning, signal processing, and real-time systems. It can provide a good balance between accuracy and performance in these applications.\\n\\nOverall, the NF4 data type offers a trade-off between precision and efficiency, making it a suitable choice for certain computational tasks where high precision is not necessary.',\n",
              "   'reference': {'must_mention': ['precision', 'storage']}},\n",
              "  '4d18fbd6-3d41-4234-8eed-e774652570a7': {'input': {'question': 'What impact does Retrieval Augmented Generation have on chatbot development?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is the helpfulness of the submission. \\n\\nThe submission provides a detailed explanation of Retrieval Augmented Generation (RAG) and its impact on chatbot development. It starts with a brief introduction of RAG, explaining its function and how it works. \\n\\nThe submission then lists four ways in which RAG improves chatbot development: improving response quality, handling open-domain queries, enabling personalization, and providing scalability and adaptability. Each point is explained in detail, providing insight into how RAG contributes to each aspect.\\n\\nThe submission is appropriate for the given input as it directly answers the question and provides relevant information. It is also insightful as it provides a comprehensive understanding of the topic, which would be helpful for someone looking to understand the impact of RAG on chatbot development.\\n\\nTherefore, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('1726e1a0-7232-4c6b-9fcb-ec38342f9036'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer is comprehensive and detailed, explaining the impact of Retrieval Augmented Generation (RAG) on chatbot development. The student correctly identifies that RAG improves response quality, handles open-domain queries, enables personalization, and provides scalability and adaptability. The student's answer aligns with the context provided, which mentions 'relevance' and 'accuracy' as key aspects of RAG's impact. The student's answer does not contradict any information in the context and provides additional details that further explain the impact of RAG on chatbot development.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('ce19ae21-b1c6-45a6-9a4a-52a058539f5d'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 5.321438,\n",
              "   'run_id': '3f2b59b3-1396-439b-8b1e-1a101a32f745',\n",
              "   'output': \"Retrieval Augmented Generation (RAG) is a technique that combines the benefits of retrieval-based and generation-based models in natural language processing. It involves using a retriever to find relevant information from a large corpus of text and then using a generator to produce a response based on that information. RAG has shown promising results in improving the performance of chatbots in various ways:\\n\\n1. Improved Response Quality: RAG models can generate more coherent and contextually relevant responses by leveraging information retrieved from a large knowledge base. This leads to more engaging and informative conversations with users.\\n\\n2. Better Handling of Open-domain Queries: RAG models excel in handling open-domain queries where the information required for generating a response may not be present in the training data. By retrieving relevant information from external sources, RAG models can provide more accurate and diverse responses to a wide range of queries.\\n\\n3. Enhanced Personalization: RAG models can personalize responses based on the user's input and context by retrieving relevant information about the user or the conversation history. This leads to more tailored and user-specific interactions, improving the overall user experience.\\n\\n4. Scalability and Adaptability: RAG models can easily scale to handle large amounts of data and adapt to new information by updating the retriever component with fresh data. This makes them suitable for applications that require continuous learning and adaptation, such as chatbot development.\\n\\nOverall, Retrieval Augmented Generation has a significant impact on chatbot development by enhancing response quality, handling open-domain queries effectively, enabling personalization, and providing scalability and adaptability to the chatbot system.\",\n",
              "   'reference': {'must_mention': ['relevance', 'accuracy']}},\n",
              "  '58e75b52-ee7b-4380-ae43-178644c5b536': {'input': {'question': 'Can QLoRA be applied to image recognition tasks?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment=\"The criterion for this task is the helpfulness of the submission. \\n\\nThe submission provides a detailed explanation of what QLoRA is and how it can potentially be applied to image recognition tasks. It explains how images can be represented as matrices and how QLoRA can be used to perform low-rank approximation on these matrices. This information is insightful and can help the user understand the potential application of QLoRA in image recognition.\\n\\nThe submission also mentions that QLoRA is a relatively new algorithm and its application to image recognition tasks may require further research. This is an important point that adds to the helpfulness of the submission by setting realistic expectations about the current state of QLoRA's application in image recognition.\\n\\nFinally, the submission offers to look up more information if the user is interested. This is a helpful offer that shows a willingness to provide further assistance.\\n\\nBased on these points, the submission is helpful, insightful, and appropriate. Therefore, it meets the criterion.\\n\\nY\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('04b59093-dd60-4b50-bd2e-78bea768f237'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer is in line with the context provided. The context suggests that QLoRA can be applied to image recognition tasks, and the student's answer confirms this. The student also provides additional information on how QLoRA could be applied to image recognition tasks, which does not conflict with the context. Therefore, the student's answer is correct.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('cd9384ab-1d23-4b23-9725-e80be837b9f8'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 4.921367,\n",
              "   'run_id': '4d19d32c-b991-400b-9184-bf2f3602517b',\n",
              "   'output': \"QLoRA (Quantum Low Rank Approximation) is a quantum machine learning algorithm that is designed for low-rank matrix approximation tasks. While QLoRA is primarily used for matrix approximation problems, it can potentially be applied to image recognition tasks as well.\\n\\nIn image recognition tasks, images can be represented as matrices where each element corresponds to a pixel value. By applying QLoRA to these image matrices, it may be possible to perform low-rank approximation on the image data. This can help in reducing the dimensionality of the image data while preserving important features, which can be beneficial for tasks like image compression, denoising, and feature extraction.\\n\\nHowever, it's important to note that QLoRA is a relatively new algorithm and its application to image recognition tasks may require further research and experimentation to determine its effectiveness and performance compared to classical image recognition algorithms and quantum machine learning algorithms specifically designed for image recognition.\\n\\nIf you would like more information on the application of QLoRA to image recognition tasks, I can look up relevant research articles or resources for you. Just let me know!\",\n",
              "   'reference': {'must_mention': ['yes', 'adaptability']}},\n",
              "  'c984fd41-3ed4-4518-bb7b-f493fcb69805': {'input': {'question': 'What advancements in optimization algorithms are proposed in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a detailed explanation of the advancements in optimization algorithms proposed in the QLoRA paper. It explains the integration of Quantized Low-Rank Adapters (QLoRA) to fine-tune large language models (LLMs) and how this contributes to a more resource-efficient and legally compliant marketplace. \\n\\nThe submission also discusses the evolution of model fine-tuning techniques, particularly low-rank adapters and quantized low-rank adapters, and how these contribute to a sustainable and economically compliant framework for LLM utilization. \\n\\nAdditionally, the submission introduces the concept of a Quantized Influence Measure (QIM) as an innovative \"AI Judge\" mechanism to enhance the precision of result selection. \\n\\nOverall, the submission is helpful as it provides a comprehensive and insightful explanation of the advancements proposed in the QLoRA paper. It is also appropriate as it directly answers the question asked in the input. \\n\\nTherefore, the submission meets the criterion.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('b44c6796-795c-40c4-91a3-8208430c99d6'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='COT Contextual Accuracy', score=1, value='CORRECT', comment=\"The student's answer mentions the integration of Quantized Low-Rank Adapters (QLoRA) for fine-tuning large language models (LLMs) in a resource-efficient manner. This aligns with the context of 'efficiency'. The student also mentions the introduction of a Quantized Influence Measure (QIM) to enhance the precision of result selection, which could be interpreted as an advancement in 'training speed'. However, the context provided is not detailed enough to fully confirm the accuracy of the student's answer. Based on the information given, the student's answer seems to be in line with the context.\\nGRADE: CORRECT\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('3a0f353b-eabd-47ca-be4e-3ba9e647a38a'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='must_mention', score=False, value=None, comment=None, correction=None, evaluator_info={}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 6.33197,\n",
              "   'run_id': '1bb63bea-d200-4aea-ad1e-6b36cb990e4c',\n",
              "   'output': 'The advancements in optimization algorithms proposed in the QLoRA paper include the integration of Quantized Low-Rank Adapters (QLoRA) to fine-tune large language models (LLMs) within a legally compliant and resource-efficient marketplace. QLoRA stands at the forefront of model refinement through parameter-efficient fine-tuning and memory optimization. The paper also discusses the evolution of model fine-tuning techniques, particularly low-rank adapters and quantized low-rank adapters, to create a sustainable and economically compliant framework for LLM utilization. Additionally, the paper introduces a Quantized Influence Measure (QIM) as an innovative \"AI Judge\" mechanism to enhance the precision of result selection, further refining the system\\'s accuracy. These advancements contribute significant insights into LLM optimization and mark a significant step forward in the development of more sophisticated and precise conversational AI systems.',\n",
              "   'reference': {'must_mention': ['efficiency', 'training speed']}}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=agent_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=f\"RAG Pipeline - Evaluation - {uuid4().hex[0:8]}\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langsmith\n",
        "\n",
        "path : https://smith.langchain.com/o/ec0ab543-3d20-5b47-b548-e29646706b91/projects/p/5d7ae3eb-f47c-46a7-b088-83c8f47e0a63?timeModel=%7B%22duration%22%3A%227d%22%7D"
      ],
      "metadata": {
        "id": "yblVtsCQRD1J"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R2OBKA9iRBVO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}