{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LangChain with Open Source LLM and Open Source Embeddings & LangSmith\n",
        "\n",
        "In the following notebook we will dive into the world of Open Source models hosted on Hugging Face's [inference endpoints](https://ui.endpoints.huggingface.co/).\n",
        "\n",
        "The notebook will be broken into the following parts:\n",
        "\n",
        "- 🤝 Breakout Room #1:\n",
        "  1. Set-up Hugging Face Inference Endpoints\n",
        "  2. Install required libraries\n",
        "  3. Set Environment Variables\n",
        "  4. Testing our Hugging Face Inference Endpoint\n",
        "  5. Creating LangChain components powered by the endpoints\n",
        "  6. Retrieving data from Arxiv\n",
        "  7. Creating a simple RAG pipeline with [LangChain v0.1.0](https://blog.langchain.dev/langchain-v0-1-0/)\n",
        "  \n",
        "\n",
        "- 🤝 Breakout Room #2:\n",
        "  1. Set-up LangSmith\n",
        "  2. Creating a LangSmith dataset\n",
        "  3. Creating a custom evaluator\n",
        "  4. Initializing our evaluator config\n",
        "  5. Evaluating our RAG pipeline"
      ],
      "metadata": {
        "id": "ctrwj6Cj24Zp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🤝 Breakout Room #1"
      ],
      "metadata": {
        "id": "AduTna3oCbP4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Set-up Hugging Face Infrence Endpoints\n",
        "\n",
        "Please follow the instructions provided [here](https://github.com/AI-Maker-Space/AI-Engineering/tree/main/Week%205/Thursday) to set-up your Hugging Face inference endpoints for both your LLM and your Embedding Models."
      ],
      "metadata": {
        "id": "ENUY6OSnDy7A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Install required libraries\n",
        "\n",
        "Now we've got to get our required libraries!\n",
        "\n",
        "We'll start with our `langchain` and `huggingface` dependencies.\n",
        "\n"
      ],
      "metadata": {
        "id": "-spIWt2J3Quk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-core langchain-community langchain_openai huggingface-hub requests -q -U"
      ],
      "metadata": {
        "id": "EwGLnp31jXJj"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can grab some miscellaneous dependencies that will help us power our RAG pipeline!"
      ],
      "metadata": {
        "id": "yPXElql-EE9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arxiv pymupdf faiss-cpu -q -U"
      ],
      "metadata": {
        "id": "FMJqq8SYt34V"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Set Environment Variables\n",
        "\n",
        "We'll need to set our `HF_TOKEN` so that we can send requests to our protected API endpoint.\n",
        "\n",
        "We'll also set-up our OpenAI API key, which we'll leverage later.\n",
        "\n"
      ],
      "metadata": {
        "id": "SpZTBLwK3TIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"HF_TOKEN\"] = getpass.getpass(\"HuggingFace Write Token: \")"
      ],
      "metadata": {
        "id": "NspG8I0XlFTt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b10a1020-1ba9-403b-d90d-d20a751287cb"
      },
      "execution_count": 31,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "HuggingFace Write Token: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"OpenAI API Key:\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giMejsXN7EKb",
        "outputId": "3efc50fc-f36a-41c8-f124-8988690ab052"
      },
      "execution_count": 32,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Testing our Hugging Face Inference Endpoint\n",
        "\n",
        "Let's submit a sample request to the Hugging Face Inference endpoint!"
      ],
      "metadata": {
        "id": "-3M7TzXs3WsJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_api_gateway = \"https://uhgw0hdx8tf3i0m6.us-east-1.aws.endpoints.huggingface.cloud\" # << YOUR ENDPOINT URL HERE"
      ],
      "metadata": {
        "id": "uyFgZVUSEexW"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> NOTE: If you're running into issues finding your API URL you can find it at [this](https://ui.endpoints.huggingface.co/) link.\n",
        "\n",
        "Here's an example:\n",
        "\n",
        "![image](https://i.imgur.com/xSCV0xM.png)"
      ],
      "metadata": {
        "id": "EvnMlmEsEiqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Direct HTTP Request Using requests\n",
        "\n",
        "- Approach: This script directly uses the requests library to send a POST request to the model's API endpoint. It manually constructs the JSON body and headers required for the request.\n",
        "\n",
        "\n",
        "- Customization: It explicitly sets parameters like max_new_tokens, top_p, and temperature in the JSON payload. These parameters control the behavior of the text generation, such as the maximum length of the generated response, how the randomness is applied, and the diversity of the generated text.\n",
        "\n",
        "\n",
        "- Authentication: The script uses an environment variable (HF_TOKEN) for authentication, included in the request headers.\n",
        "\n",
        "\n",
        "- Flexibility: This method offers high flexibility in terms of specifying HTTP headers, request parameters, and handling the response."
      ],
      "metadata": {
        "id": "mhEtwBtxpGRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "max_new_tokens = 256\n",
        "top_p = 0.9\n",
        "temperature = 0.1\n",
        "\n",
        "prompt = \"Hello! How are you?\"\n",
        "\n",
        "json_body = {\n",
        "    \"inputs\" : prompt,\n",
        "    \"parameters\" : {\n",
        "        \"max_new_tokens\" : max_new_tokens,\n",
        "        \"top_p\" : top_p,\n",
        "        \"temperature\" : temperature\n",
        "    }\n",
        "}\n",
        "\n",
        "headers = {\n",
        "  \"Authorization\": f\"Bearer {os.environ['HF_TOKEN']}\",\n",
        "  \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "response = requests.post(model_api_gateway, json=json_body, headers=headers)\n",
        "print(response.json())"
      ],
      "metadata": {
        "id": "-fVaR1onmtkz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "516f7673-c269-411f-dd84-d3e4e43a3105"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'generated_text': \" I'm doing well, thanks for asking! *smiling*\\n\\nI hope you're having a great day and enjoying the beautiful weather. *winking*\\n\\nI just wanted to reach out and say hello, and see how you're doing. It's always nice to connect with someone new. *nodding*\\n\\nSo, tell me a little bit about yourself. What do you like to do in your free time? Do you have any hobbies or interests that you enjoy? *curious*\\n\\nAnd hey, if you want to chat more, I'm all ears! *grinning* I'm always up for a good conversation. *winking*\\n\\nTake care, and talk to you soon! *smiling*\"}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5: Creating LangChain components powered by the endpoints\n",
        "\n",
        "We're going to wrap our endpoints in LangChain components in order to leverage them, thanks to LCEL, as we would any other LCEL component!"
      ],
      "metadata": {
        "id": "mXTBnBTy3b62"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using langchain.llms Library for HuggingFaceEndpoint\n",
        "\n",
        "- Approach: This script utilizes the langchain.llms library's HuggingFaceEndpoint class, abstracting away the direct use of HTTP requests. It's a higher-level interface compared to the raw requests approach.\n",
        "\n",
        "\n",
        "- Simplicity and Readability: The use of a dedicated class for the Hugging Face endpoint makes the code simpler and more readable. It hides the complexity of constructing HTTP requests manually.\n",
        "\n",
        "\n",
        "- Configuration: It assumes some default parameters for the text generation task (\"text-generation\") and likely abstracts the handling of parameters such as max_new_tokens, top_p, and temperature inside the invoke method or the class initialization, offering a simpler interface for common tasks.\n",
        "\n",
        "\n",
        "- Authentication: Similar to the first script, it uses an environment variable for authentication but wrapped within the class's functionality.\n",
        "\n"
      ],
      "metadata": {
        "id": "ptVKYL4bpS9H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HuggingFaceEndpoint for LLM\n",
        "\n",
        "We can use the `HuggingFaceEndpoint` found [here](https://github.com/langchain-ai/langchain/blob/master/libs/community/langchain_community/llms/huggingface_endpoint.py) to power our chain - let's look at how we would implement it."
      ],
      "metadata": {
        "id": "fd5DaxGEFohF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "8vc7K1rFhSVt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "879337a5-fa95-40d0-b19f-9d942fcdd1a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token has not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from langchain.llms import HuggingFaceEndpoint\n",
        "\n",
        "endpoint_url = (\n",
        "    model_api_gateway\n",
        ")\n",
        "\n",
        "hf_llm = HuggingFaceEndpoint(\n",
        "    endpoint_url=endpoint_url,\n",
        "    huggingfacehub_api_token=os.environ[\"HF_TOKEN\"],\n",
        "    task=\"text-generation\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can use our endpoint like we would any other LLM!"
      ],
      "metadata": {
        "id": "t-PBb3MPFN_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hf_llm.invoke(\"Hello, how are you?\")"
      ],
      "metadata": {
        "id": "mMJrWnKISFqb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "ab7f729b-898e-447f-f00b-69f2a81f7d4e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n* 你好，我很好，谢谢 (nǐ hǎo, wǒ hěn hǎo, xiè xiè) - Hello, I'm very good, thank you.\\n* 我很愉快 (wǒ hěn yúkuài) - I'm very happy.\\n* 我很感谢 (wǒ hěn gǎnxiè) - I'm very grateful.\\n\\nHow do you like China?\\n* 中国很好 (Zhōngguó hěn hǎo) - China is very good.\\n* 我喜欢中国的文化 (wǒ xǐ huān Zhōngguó de wénhuà) - I like China's culture.\\n* 中国有很多好的食物 (Zhōngguó yǒu hěn duō hǎo de shíwù) - China has many delicious foods.\\n\\nWhat do you want to do in China?\\n* 我想去中国旅游 (wǒ xiǎng qù Zhōngguó lǚyóu) - I want to travel in China.\\n* 我想吃中国的菜 (wǒ xiǎng chī Zhōngguó de cài) - I want to eat Chinese food.\\n* 我想学中国语言 (wǒ xiǎng xué Zhōngguó yǔyīng) - I want to learn Chinese language.\\n\\nCan you speak Chinese?\\n* 我可以说中文 (wǒ kě yǐ shuō Zhōngwén) - I can speak Chinese.\\n* 我不能说中文 (wǒ bù néng shuō Zhōngwén) - I cannot speak Chinese.\\n\\nWhere are you from?\\n* 我来自美国 (wǒ lái zì Mēiguó) - I come from the United States.\\n* 我来自中国 (wǒ lái zì Zhōngguó) - I come from China.\\n* 我来自其他地\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Key Differences:\n",
        "- Level of Abstraction: The first script interacts with the API at a lower level, offering more control over the request's specifics. The second script uses a higher-level abstraction that simplifies interaction but might offer less granularity in controlling request parameters.\n",
        "\n",
        "- Ease of Use: For users familiar with Python but not with making HTTP requests, the second approach might be more accessible and straightforward.\n",
        "\n",
        "- Customizability: The first script may be more suitable for advanced use cases requiring specific request headers or non-standard API parameters.\n",
        "\n",
        "- Dependency: The first script relies on the widely used requests library, which is a common choice for HTTP interactions in Python. The second script depends on the langchain.llms library, which is specifically designed for working with language models and might not be as widely adopted.\n",
        "\n",
        "- Both scripts are valid for their purposes, with the choice between them depending on the user's specific needs, preferences for code readability and simplicity, and the level of control required over the API interaction."
      ],
      "metadata": {
        "id": "bxaCD1ihpoY_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### HuggingFace Inference API Embeddings\n",
        "\n",
        "Now we can leverage the `HuggingFaceInferenceAPIEmbeddings` module in LangChain to connect to our Hugging Face Inference Endpoint hosted embedding model."
      ],
      "metadata": {
        "id": "1EBtSBMj3-Hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_api_gateway = \"https://vpuv173f4vtfog2r.us-east-1.aws.endpoints.huggingface.cloud\" # << Embedding Endpoint API URL"
      ],
      "metadata": {
        "id": "wrZJHVGkGLZr"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
        "\n",
        "embeddings_model = HuggingFaceInferenceAPIEmbeddings(api_key=os.environ[\"HF_TOKEN\"], api_url=embedding_api_gateway)"
      ],
      "metadata": {
        "id": "4asz9Ofn0MtP"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings_model.embed_query(\"Hello, welcome to HF Endpoint Embeddings\")[:10]"
      ],
      "metadata": {
        "id": "HvF_eMZZKnlm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "445d5352-6358-4837-9adf-8fe2cbeea587"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.01926689,\n",
              " 0.01546007,\n",
              " -0.046256967,\n",
              " -0.021581108,\n",
              " -0.009921011,\n",
              " 0.00024049378,\n",
              " -0.033302825,\n",
              " -0.0010723798,\n",
              " 0.027798,\n",
              " 0.011502621]"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "np.size(embeddings_model.embed_query(\"Hello, welcome to HF Endpoint Embeddings\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LrO9po6fT3c",
        "outputId": "1a0ef589-d813-4ffe-d058-4838a8563802"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(embeddings_model.embed_query(\"Hello\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D89mQlaNgEI6",
        "outputId": "fb043b22-b698-49e1-cfd2-dd99d884ee1a"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ❓ Question #1\n",
        "\n",
        "What is the embedding dimension of your selected embeddings model?\n",
        "\n",
        "\n",
        "####  Answer #1 :\n",
        "embedding dimension is 1024"
      ],
      "metadata": {
        "id": "EtbNzDF-e7JI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 6: Retrieving data from Arxiv\n",
        "\n",
        "We'll leverage the `ArxivLoader` to load some papers about the \"QLoRA\" topic, and then split them into more manageable chunks!"
      ],
      "metadata": {
        "id": "P9pLgHfR3uY9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import ArxivLoader\n",
        "\n",
        "docs = ArxivLoader(query=\"QLoRA\", load_max_docs=5).load()"
      ],
      "metadata": {
        "id": "7yO05R6mtyCB"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size = 500,\n",
        "    chunk_overlap = 0,\n",
        "    length_function = len,\n",
        ")\n",
        "\n",
        "split_chunks = text_splitter.split_documents(docs)"
      ],
      "metadata": {
        "id": "4F249yWeuCKd"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(split_chunks)"
      ],
      "metadata": {
        "id": "d9BO1Y1Xur0e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "570e30eb-fe54-433e-fd99-b4c4a6623d7e"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "528"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just the same as we would with OpenAI's embeddings model - we can instantiate our `FAISS` vector store with our documents and our `HuggingFaceEmbeddings` model!\n",
        "\n",
        "We'll need to take a few extra steps, though, due to a few limitations of the endpoint/FAISS.\n",
        "\n",
        "We'll start by embeddings our documents in batches of `32`."
      ],
      "metadata": {
        "id": "3sZBBjdM4Or8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = []\n",
        "for i in range(0, len(split_chunks) - 1, 32):\n",
        "  embeddings.append(embeddings_model.embed_documents([document.page_content for document in split_chunks[i:i+32]]))"
      ],
      "metadata": {
        "id": "FBCTm-JZ0mVr"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embeddings = [item for sub_list in embeddings for item in sub_list]"
      ],
      "metadata": {
        "id": "4wLY8FDGNDym"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ❓ Question #2\n",
        "\n",
        "Why do we have to limit our batches when sending to the Hugging Face endpoints?\n",
        "\n",
        "\n",
        "#### Answer #2\n",
        "Limiting batches when sending to the Hugging Face can prevent the overload in shared infrastructure and manage the computational resource effectively, ensure the steady flow of requests , minimize the error due to sudden spikes in demand."
      ],
      "metadata": {
        "id": "Xgc_e-9QHJTm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create text/embedding pairs which we want use to set-up our FAISS VectorStore!"
      ],
      "metadata": {
        "id": "xn4lECg2TTza"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "text_embedding_pairs = list(zip([document.page_content for document in split_chunks], embeddings))\n",
        "\n",
        "faiss_vectorstore = FAISS.from_embeddings(text_embedding_pairs, embeddings_model)"
      ],
      "metadata": {
        "id": "6C1bw7srOVJX"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(text_embedding_pairs))\n",
        "print(len(text_embedding_pairs))\n",
        "print(text_embedding_pairs[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXIywp1MihhA",
        "outputId": "faa9d235-604e-499b-a274-ba522f7f846a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'>\n",
            "528\n",
            "('Adapters (LoRA). Our best model family, which we name Guanaco, outperforms\\nall previous openly released models on the Vicuna benchmark, reaching 99.3%\\nof the performance level of ChatGPT while only requiring 24 hours of finetuning\\non a single GPU. QLORA introduces a number of innovations to save memory\\nwithout sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that\\nis information theoretically optimal for normally distributed weights (b) Double', [0.026077243, -0.0033466534, -0.017917924, 0.003309633, 0.010076981, 0.016007666, 0.0071671694, 0.023648696, 0.02403371, 0.05265796, 0.058048148, 0.006845091, -0.006697009, -0.045905422, -0.007293039, 0.008796072, 0.0026913905, -0.06343833, -0.011306061, 0.00933657, 0.004853388, 0.003381823, -0.060565542, -0.027069392, -0.01580035, 0.04190721, 0.051325224, -0.0009167202, 0.083932884, 0.019280277, -0.006867303, -0.036250476, 0.013453251, -0.044394985, -0.05076251, -0.031008372, 0.02592916, 0.008485099, -0.021353425, -0.04741586, 0.030978756, -0.03859017, 0.043476876, -0.018376978, -0.04377304, -0.009388399, 0.00157985, -0.048037805, 0.016185364, -0.077298805, -0.019161811, 0.0015567121, 0.032814972, -0.01514879, -0.027306322, -0.019576441, -0.009862262, 0.019487591, -0.014400975, 0.0120983, 0.053309523, 0.024551997, 0.034562342, -0.038057078, 0.014001154, 0.061602116, -0.014267702, -0.008995982, 0.01614094, -0.014549057, -0.007952004, 0.040219072, -0.02764691, -0.046793915, -0.024640847, -0.00529023, 0.01580035, 0.0061454033, -0.025855118, 0.0059454925, 0.014482421, 0.0033262921, -0.01867314, 0.0141196195, -0.05508651, -0.041433346, 0.007863155, 0.00303383, 0.012039067, -0.0288908, -0.02338215, 0.013845668, 0.02518875, -0.011283849, 0.0049792575, 0.016733266, -0.008899729, 0.04584619, 0.010247275, -0.008448078, 0.010217658, 0.060298994, -0.0035687764, 0.05085136, -0.0027765376, 0.019857798, 0.0030764036, 0.021057261, -0.021664398, -0.05621193, 0.014400975, 0.013882688, -0.01572631, 0.023515424, -0.01951721, 0.063793726, -0.034295794, 0.04714931, -0.012905347, -0.042914167, -0.0037501769, 0.02650668, 0.0126388, -0.005031086, -0.004564628, -0.08446598, -0.009728988, 0.02246404, -0.050170183, -0.014815605, -0.00619353, -0.03373308, -0.016777692, 0.033377685, 0.0014215873, 0.0051976787, 0.006400845, 0.058048148, 0.018910073, -0.0139049, 0.04492808, -0.020257618, 0.00529023, 0.088019945, -0.042766083, 0.02732113, 0.041166797, -0.04741586, -0.025973585, 0.035924695, -0.010062172, 0.015400529, -0.023796778, 0.043151096, 0.034266178, -0.047001228, -0.007130149, 0.035036203, 0.027928267, 0.003093063, 0.0214867, 0.00843327, -0.02271578, 0.007070916, -0.034177326, 0.036428176, -0.0037335176, -0.033229604, 0.005371675, -0.01719232, 0.04409882, -0.008240764, -0.01883603, 0.020687057, 0.019635674, 0.04329918, 0.04913361, -0.008536927, 0.027128624, 0.03669472, -0.00086905627, -0.008166723, -0.022286342, 0.050318267, 0.017547717, -0.0077743055, 0.0034392045, -0.029246196, 0.0055863936, -0.012076088, -0.017888306, 0.037642445, -0.010602672, 0.029853333, -0.01662961, 0.007596607, -0.048630133, -0.010017748, -0.04632005, -0.087783016, -0.029201772, 0.07013164, 0.0033225901, 0.016777692, -0.010706329, 0.040633705, 0.006126893, 0.03130454, -0.01702943, -0.0104471855, 0.0635568, -0.022997135, -0.031719167, -0.03266689, 0.024640847, 0.0036613278, -0.00382792, 0.024315067, -0.018495442, 0.014919262, 0.00440544, 0.023885628, 0.022937903, 0.020213194, -0.021545932, 0.0064415676, 0.02625494, 0.014415784, 0.03613201, 0.0104471855, -0.016985007, 0.06032861, -0.013112661, 0.029409086, 0.0409891, 0.028535403, 0.015296872, 0.028224431, -0.013630949, 0.01424549, -0.020716673, 0.01654076, 0.0043165907, 0.005023682, 0.046853147, 0.0044387584, -0.005519757, -0.034443874, -0.05209525, 0.052361798, -0.008033449, 0.024048518, -0.020849947, -0.0040796595, -0.012187149, 0.012609183, 0.0007427238, 0.035036203, -0.018376978, -0.0037464749, -0.020361276, 0.01662961, -0.0024489062, -0.0071930834, -0.004231443, 0.045579642, 0.01280169, 0.017059047, 0.012453697, -0.031156454, -0.042825315, -0.05973628, -0.040041376, -0.019991072, -0.03870864, 0.013519888, 0.030830674, -0.019191429, -0.0029098114, -0.03248919, -0.0019546824, 0.002147189, 0.00096345856, 0.044246905, 0.039715596, -0.00052476564, -0.032814972, 0.023234067, 0.002645115, 0.03438464, -0.0074041006, 0.042825315, -0.0010078831, -0.02123496, -0.01646672, -0.028698293, -0.010958069, 0.028461361, -0.038382858, -0.02692131, -0.0058418354, -0.016940583, -0.013090449, -0.019487591, -0.033851545, 0.055323437, 0.03604316, -0.01762176, 0.040515237, 0.03891595, -0.06758463, 0.022952711, -0.0031226794, 0.04335841, -0.028920416, 0.09512788, 0.04205529, -0.028905608, -0.038412474, 0.0039167693, 0.0051310416, -0.04898553, 0.021990178, -0.017547717, -0.06249061, 0.04377304, 0.011002493, -0.061839048, 0.021294193, -0.008966366, -0.014304722, 0.013120066, -0.05751505, -0.0016205725, -0.010358336, 0.044572685, 0.00241929, 0.012416677, -0.04187759, 0.013038621, 0.052539498, -0.06260908, 0.0046090526, 0.0371982, 0.06592611, 0.016422294, 0.034266178, -0.023248876, -0.018362168, 0.021945754, -0.044335753, 0.03284459, -0.002199018, -0.0010421271, 0.046468135, 0.0012013153, -0.026876884, 0.018465826, 0.020257618, 0.009166276, 0.02239, 0.037405517, 0.033762697, 0.015948432, -0.016674034, -0.00798162, -0.02056859, 0.028772334, 0.0446023, -0.029927375, 0.031541467, -0.006693307, -0.052717194, 0.008055661, -0.039212115, -0.011017301, 0.033140752, -0.0034262475, 0.057870448, -0.0391825, -0.0007209743, 0.040189456, 0.031067606, 0.028431745, 0.021945754, 0.025173942, 0.008729435, 0.023189643, -0.028135581, 0.0041611046, 0.025484914, -0.0052606133, -0.040870633, -0.014267702, -0.028135581, -0.023485806, 0.037583213, 0.048304353, 0.03349615, -0.03062336, 0.07528489, 0.020183578, 0.026906502, 0.039774828, 0.0004826548, -0.0024785227, -0.045757342, 0.028372513, -0.017221937, -0.005630818, -0.030134689, -0.024818545, -0.027869035, 0.015415337, 0.02206422, -0.002639562, -0.044483837, 0.030830674, 0.007996429, 0.05111791, -0.021664398, -0.034118094, -0.0076188194, 0.029779293, 0.0043313988, -0.048304353, 0.005752986, -0.042825315, 0.017325595, 0.043980356, -0.03210418, -0.029335046, -0.037257433, 0.021042453, -0.015563419, 0.050170183, 0.033140752, 0.008773859, -0.0028987054, -0.04024869, 0.0042499537, 0.026240133, -0.028135581, -0.008388846, 0.029601593, 0.015755925, 0.0005437386, -0.0072782305, 0.0063860365, 0.011224616, 0.040841017, -0.039893292, 0.0017686544, -0.016081706, -0.020257618, -0.033970013, -0.027883843, -0.007596607, 0.0075484803, -0.0058381334, -0.020509358, -0.03784976, 0.028209623, -0.0029690443, -0.030978756, 0.033940397, 0.00060065766, -0.02403371, 0.025455298, 0.027987499, -0.041699894, 0.029068498, 0.050644048, -0.04092987, 0.03482889, 0.00019597728, 0.02470008, -0.0061491053, -0.0012522185, -0.023189643, -0.04848205, 0.05022942, 0.045165014, 0.0086109685, -0.012720245, -0.041788742, -0.034443874, 0.0241966, -0.04057447, 0.02338215, 0.00042920644, -0.056833874, -0.00017237672, -0.000994926, -0.010225062, -0.007318953, 0.007019087, 0.006249061, 0.017755033, 0.02197537, -0.002141636, 0.01719232, -0.030060647, 0.010076981, -0.061424416, -0.022937903, -0.05319106, -0.010543439, -0.02616609, -0.002073148, -0.026684377, -0.054286864, -0.01785869, 0.005978811, -0.016940583, 0.0068339845, 0.015370913, -0.013090449, -0.053161442, 0.045135397, 0.0020065112, -0.065985344, -0.062135212, 0.03405886, -0.0055086506, 0.0214867, -0.030741826, 0.006619266, -0.012994196, -0.059292037, 0.022730589, -0.015126578, 0.015163598, -0.030253155, -0.0669923, -0.019828182, -0.009610523, 0.038175542, -0.04815627, 0.010358336, -0.04552041, 0.036576256, -0.018776799, -0.025247982, -0.04800819, 0.011157979, 0.009343974, 0.049992487, 0.010410165, 0.009810433, 0.02732113, 0.0505552, 0.00967716, 0.04931131, -0.045698106, -0.042410687, 0.00810749, 0.02921658, 0.0032911226, -0.023500614, -0.08470291, 0.026876884, -0.054553412, 0.03302229, -0.021249767, 0.009084831, 0.004927429, -0.061779816, 0.02370793, -0.007459631, 0.008418462, -0.003687242, 0.026195707, 0.055678833, -0.025410872, 0.013438442, -0.019620866, -0.023248876, -0.05508651, 0.017577335, -0.06657767, -0.0050458945, -0.02296752, -0.03210418, 0.035095435, 0.0028320684, 0.00619353, 0.0778319, -0.00529023, -0.018228896, 0.004201827, 0.0014872986, 0.005967705, -0.03779053, 0.013638353, -0.032726124, -0.025647804, -0.025736652, -0.09044849, -0.06829542, -0.013023812, 0.02253808, 0.07410023, -0.013667969, 0.031067606, 0.049814787, -0.0150599405, -0.027943075, 0.03163032, -0.014326935, 0.0066414783, 0.04765279, 0.008292592, -0.046023887, 0.033555385, -0.02797269, 0.0069117276, 0.019324701, 0.045727722, -0.0429734, -0.044335753, 0.04131488, -0.00011690381, -0.028342897, -0.038471706, -0.032193027, 0.01061748, 0.009232913, -0.016244596, 0.033644233, -0.036487408, -0.020361276, -0.015829967, 0.01959125, 0.002199018, 0.01686654, 0.04362496, 0.012542546, -0.025544146, 0.0034355025, 0.035924695, -0.00068302825, 0.010387952, 0.0062305504, -0.005008874, -0.009795625, 0.007370782, 0.0012142725, -0.041374113, -0.0012337082, -0.0052309968, 0.03382193, 0.028105965, 0.0015567121, -0.012409273, -0.020790715, -0.023693122, -0.04395074, -0.052154485, 0.030386427, -0.016836924, 0.011165383, -0.0013928964, 0.0391825, 0.024152176, -0.010661905, -0.029172156, -0.058314696, -0.008766455, -0.08878997, -0.058462776, -0.03414771, -0.061602116, -0.006252763, 0.044631917, 0.003168955, -0.026847268, 0.0068191765, 0.007929792, -0.033940397, 0.0047571347, -0.016007666, -0.0075003537, -0.03767206, -0.03577661, -0.018939689, -0.03571738, 0.0024951817, 0.03062336, -0.007826135, 0.0222123, 0.03192648, 0.017221937, -0.006485992, 0.011920602, -0.016111322, -0.017962348, -0.07552183, -0.007929792, -0.0054827365, 0.03242996, 0.019309893, -0.009743796, -0.0058714515, 0.005960301, 0.007900175, -0.02338215, 0.00029708951, 0.020390892, 0.013823455, -0.017177513, -0.037642445, 0.020716673, -0.05061443, -0.0071190423, -0.012053875, -0.02970525, -0.052776426, -0.00017214533, 0.03284459, 0.02840213, 0.05319106, 0.036428176, 0.005752986, -0.036842804, 0.03479927, -0.009529077, 0.03891595, -0.0063119954, 0.02016877, -0.020849947, 0.03669472, 0.049103994, -0.011298657, 0.011824348, -0.03746475, -0.0018806416, -0.034355026, -0.014201065, 0.02139785, 0.05168062, -0.00061916787, 0.021101687, -0.023248876, -0.0038427282, -0.0319561, -0.02921658, -0.057663135, -0.028328089, -0.001912109, -0.066281505, 0.006700711, 0.024181793, -0.02255289, -0.01197243, 0.034858506, -0.019324701, 0.0150007075, 0.03844209, -0.013645757, 0.0150155155, -0.0038094097, -0.02970525, -0.0047830488, 0.014326935, -0.083340555, -0.03130454, 0.006778454, 0.04232184, 0.010262083, -0.0019102579, -0.051917553, 0.017399635, 0.01514879, 0.030712208, -0.045757342, 0.02443353, -0.015785541, 0.031423002, -0.006852495, -0.02378197, -0.0024859267, 0.009380995, 0.0056863492, 0.031719167, -0.0409891, 0.046379287, 0.028298471, -0.040633705, 0.060713623, 0.016674034, 0.0021601464, -0.0017168258, 0.019102579, 0.0052458053, 0.032548424, 0.01180954, 0.03826439, -0.053487223, 0.010565652, 0.022079028, -0.005305038, 0.007966812, 0.029764483, 0.028875992, 0.002946832, -0.00046021113, -0.0150451325, -0.015385721, -0.0008995982, 0.007370782, -0.05573807, -0.0048052613, -0.0353916, 0.0012762818, -0.018214088, 0.03192648, -0.0053309524, -0.017044239, -0.026654761, 0.026136475, 0.008625777, -0.0037316666, 0.01621498, -0.015563419, 0.021545932, 0.053753767, 0.011557801, 0.0060195336, 0.0026913905, 0.029779293, -0.029601593, 0.0131867025, 0.03497697, -0.0048867064, -0.0036261582, -0.014038174, -0.034710422, 0.05274681, -0.048955914, -0.053131826, -0.022937903, -0.017325595, -0.027143432, -0.036457792, -0.012253786, -0.074514866, 0.004057447, 0.0594105, 0.026743611, 0.0069006216, 0.015859583, 0.0076928604, 0.010676713, 0.043239947, 0.03151185, -0.042766083, 0.067525394, 0.07019087, -0.0113430815, 0.021545932, -0.008581352, -0.011617034, -0.002071297, -1.2472728e-05, 0.0054864385, 0.0518287, 0.015518994, -0.032696508, 0.021442275, 0.017340403, -0.020316852, -0.042677235, 0.0214867, 0.005379079, -0.046556983, -0.009743796, 0.060121294, 0.016925773, 0.05061443, -0.016659226, -0.03784976, -0.019132195, 0.022819437, -0.013134874, 0.042766083, -0.04436537, -0.013149682, -0.042203374, -0.05088098, -0.025011051, -0.017518101, 0.007007981, -0.035213903, 0.043003015, 0.05908472, 0.006060256, -0.0043165907, -0.03136377, -0.053161442, 0.05325029, 0.05529382, -0.016422294, 0.006400845, 0.051265992, 0.043417644, 0.007855751, -0.005475332, 0.003389227, -0.027380364, -0.017399635, -0.011624438, 0.030238345, 0.0057751983, -0.021545932, -0.028387321, 0.030001415, -0.01851025, -0.031896863, -0.0505552, -0.0012392614, -0.04377304, 0.02865387, -0.033229604, -0.002074999, 0.011061726, -0.014178853, 0.002732113, -0.04403959, 0.2342065, 0.04362496, 0.04750471, 0.044394985, 0.028772334, 0.046675447, 0.031867247, -0.0011883581, 0.017488485, -0.032696508, 0.039004803, 0.015208023, 0.018702758, 0.003478076, -0.017799458, 0.026047625, -0.038501322, 0.036576256, 0.027543254, -0.017295979, -0.015918816, 0.019606058, 0.015770733, -0.0029931075, 0.015356104, 0.030223537, -0.021027645, -0.029897757, -0.009936303, -0.01267582, 0.02016877, -0.04963709, 0.041966442, -0.024211409, -0.029231388, 0.05695234, 0.0058529414, -0.050022103, -0.02116092, 0.002587733, -0.000569653, -0.001179103, -0.005745582, -0.0086627975, -0.029083306, 0.031659935, -0.020953605, 0.04335841, 0.031659935, -0.053279907, 0.02650668, -0.062194444, 0.0087072225, -0.010010344, -0.054138783, 0.025973585, -0.023248876, -0.04664583, -0.002369312, 0.06509685, -0.018939689, 0.0022619527, 0.0032652083, 0.012394464, -0.0095438855, 0.022419617, 0.002060191, -0.018880457, -0.00025428456, -0.018895265, -0.0063786325, -0.012594375, 0.014267702, -0.014882241, 0.019058155, 0.024507573, -0.02872791, 0.017266363, -0.009062619, 0.0006765497, -0.017295979, -0.0260032, -0.021412658, 0.0034114392, 0.026447447, 0.019324701, -0.044957697, -0.03438464, -0.008781263, 0.025632996, 0.04409882, 0.06722923, 0.0167925, -0.01565227, 0.004575734])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we set up FAISS as a retriever."
      ],
      "metadata": {
        "id": "NXbexmFSTZKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "faiss_retriever = faiss_vectorstore.as_retriever(search_kwargs={\"k\" : 2})"
      ],
      "metadata": {
        "id": "BSUZYfvAPxTF"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it out!"
      ],
      "metadata": {
        "id": "ce1ZWj8aTchK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "faiss_retriever.get_relevant_documents(\"What optimizer does QLoRA use?\")"
      ],
      "metadata": {
        "id": "0DwHoaIDQQ9E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "660bccfd-31b0-4c11-8238-abf2b5e8c5e5"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='Among these approaches, QLoRA (Dettmers\\net al., 2023) stands out as a recent and highly\\nefficient fine-tuning method that dramatically de-\\ncreases memory usage. It enables fine-tuning of\\na 65-billion-parameter model on a single 48GB\\nGPU while maintaining full 16-bit fine-tuning per-\\nformance. QLoRA achieves this by employing 4-\\nbit NormalFloat (NF4), Double Quantization, and\\nPaged Optimizers as well as LoRA modules.\\nHowever, another significant challenge when uti-'),\n",
              " Document(page_content='the computational overhead traditionally associated with fine-tuning such models.\\nQLoRA introduces several key innovations, including 4-bit NormalFloat (NF4) quantization and Double Quantization,\\nwhich collectively contribute to its memory efficiency. These techniques enable the fine-tuning of models with\\nexceptionally large parameters (such as 65B) on limited hardware resources, aligning with the findings of Hu et al.\\n[2021].\\n4')]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Template\n",
        "\n",
        "Now that we have our LLM and our Retiever set-up, let's connect them with our Prompt Template!"
      ],
      "metadata": {
        "id": "Xm0IjkpFSdmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import ChatPromptTemplate\n",
        "\n",
        "RAG_PROMPT_TEMPLATE = \"\"\"\\\n",
        "Using the provided context, please answer the user's question. If you don't know, say you don't know.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\"\"\"\n",
        "\n",
        "rag_prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)"
      ],
      "metadata": {
        "id": "Gqpayd-kTyiq"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ❓ Question #3\n",
        "\n",
        "Does the ordering of the prompt matter?\n",
        "\n",
        "#### Answer #3\n",
        "\n",
        "- The order of elements within the prompt (like context, questions, and instructions) can significantly influence the output of model because most of transformer-based LLM models are sensitive to the sequence and structure of their input.\n",
        "\n",
        "\n",
        "- Putting the Context before question helps LLM to understand the background information first. Instruction clarity and the consistency in prompt can enhance the performance of language models.\n",
        "\n"
      ],
      "metadata": {
        "id": "NikHqHljIIdK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 7: Creating a simple RAG pipeline with LangChain v0.1.0\n",
        "\n",
        "All that's left to do is set up a RAG chain - and away we go!"
      ],
      "metadata": {
        "id": "Gwy1YOy34aXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnableParallel\n",
        "from langchain.schema import StrOutputParser\n",
        "\n",
        "retrieval_augmented_qa_chain = (\n",
        "    {\n",
        "        \"context\": itemgetter(\"question\") | faiss_retriever,\n",
        "        \"question\": itemgetter(\"question\"),\n",
        "    }\n",
        "    | rag_prompt\n",
        "    | hf_llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "i0q8CUu809M-"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's test it out!"
      ],
      "metadata": {
        "id": "sHyy5p484iUD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is QLoRA?\"})"
      ],
      "metadata": {
        "id": "OezUhZGrUr63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "c02a26dd-7f10-43c8-9757-8e7eb7e761f5"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nAnswer:\\nQLoRA is a method for fine-tuning large language models (LLMs) that involves using low-rank matrices and quantization techniques to reduce the computational complexity of the models. It is designed to make the finetuning of high-quality LLMs more widely and easily accessible, particularly in the hands of large corporations that do not release models or source code for auditing.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🤝 Breakout Room #2"
      ],
      "metadata": {
        "id": "LGsV8x_ZIWZ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 1: Set-up LangSmith\n",
        "\n",
        "We'll be moving through this notebook to explain what visibility tools can do to help us!\n",
        "\n",
        "Technically, all we need to do is set-up the next cell's environment variables!"
      ],
      "metadata": {
        "id": "YrKQSs_r4gl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from uuid import uuid4\n",
        "\n",
        "unique_id = uuid4().hex[0:8]\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIE1 - {unique_id}\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass('Enter your LangSmith API key: ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1S5X3EE847PO",
        "outputId": "0026f1e1-f2ee-40e3-85bd-000db57a2146"
      },
      "execution_count": 56,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your LangSmith API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see what happens on the LangSmith project when we run this chain now!"
      ],
      "metadata": {
        "id": "ou1fLN-MJGfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "retrieval_augmented_qa_chain.invoke({\"question\" : \"What is QLoRA?\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "1Yr8j5hqJGET",
        "outputId": "de34070d-8b45-4ba1-ccfe-c43c0d67544e"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nAnswer:\\nQLoRA is a method for fine-tuning large language models (LLMs) that involves using low-rank matrices and quantization techniques to improve the efficiency and accuracy of the models. It is described in a research paper as a way to make the fine-tuning of high-quality LLMs more widely and easily accessible, and is facilitated by the application of QLoRA in Viz, which clarifies the technical complexities and operational benefits of this integration.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get *all of this information* for \"free\":\n",
        "\n",
        "![image](https://i.imgur.com/8Wcpmcj.png)\n",
        "\n",
        "> NOTE: We'll walk through this diagram in detail in class."
      ],
      "metadata": {
        "id": "zmaxEfcWJWXc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####🏗️ Activity #1:\n",
        "\n",
        "Please describe the trace of the previous request and answer these questions:\n",
        "\n",
        "1. How many tokens did the request use?\n",
        "2. How long did the `HuggingFaceEndpoint` take to complete?"
      ],
      "metadata": {
        "id": "JsFaAg1TJ8JE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2: Creating a LangSmith dataset\n",
        "\n",
        "Now that we've got LangSmith set-up - let's explore how we can create a dataset!\n",
        "\n",
        "First, we'll create a list of questions!"
      ],
      "metadata": {
        "id": "0XdbE0m3JgJp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langsmith import Client\n",
        "\n",
        "questions = [\n",
        "    \"What optimizer is used in QLoRA?\",\n",
        "    \"What data type was created in the QLoRA paper?\",\n",
        "    \"What is a Retrieval Augmented Generation system?\",\n",
        "    \"Who authored the QLoRA paper?\",\n",
        "    \"What is the most popular deep learning framework?\",\n",
        "    \"What significant improvements does the LoRA system make?\"\n",
        "]"
      ],
      "metadata": {
        "id": "-KVSO6Eh5DpC"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can create our dataset through the LangSmith `Client()`."
      ],
      "metadata": {
        "id": "urLbc0B8K6QZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client = Client()\n",
        "dataset_name = \"QLoRA RAG Dataset\"\n",
        "\n",
        "dataset = client.create_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    description=\"Questions about the QLoRA Paper to Evaluate RAG over the same paper.\"\n",
        ")\n",
        "\n",
        "client.create_examples(\n",
        "    inputs=[{\"question\" : q} for q in questions],\n",
        "    dataset_id=dataset.id\n",
        ")"
      ],
      "metadata": {
        "id": "NUH0m7AuKyn7"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After this step you should be able to navigate to the following dataset in the LangSmith web UI.\n",
        "\n",
        "![image](https://i.imgur.com/CdFYGTB.png)"
      ],
      "metadata": {
        "id": "2jxaByg9LFfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 3: Creating a custom evaluator\n",
        "\n",
        "Now that we have a dataset - we can start thinking about evaluation.\n",
        "\n",
        "We're going to make a `StringEvaluator` to measure \"dopeness\".\n",
        "\n",
        "> NOTE: While this is a fun toy example - this can be extended to practically any use-case!"
      ],
      "metadata": {
        "id": "MbVQaJi3LsdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from typing import Any, Optional\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain.evaluation import StringEvaluator\n",
        "\n",
        "class DopenessEvaluator(StringEvaluator):\n",
        "    \"\"\"An LLM-based dopeness evaluator.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
        "\n",
        "        template = \"\"\"On a scale from 0 to 100, how dope (cool, awesome, lit) is the following response to the input:\n",
        "        --------\n",
        "        INPUT: {input}\n",
        "        --------\n",
        "        OUTPUT: {prediction}\n",
        "        --------\n",
        "        Reason step by step about why the score is appropriate, then print the score at the end. At the end, repeat that score alone on a new line.\"\"\"\n",
        "\n",
        "        self.eval_chain = PromptTemplate.from_template(template) | llm\n",
        "\n",
        "    @property\n",
        "    def requires_input(self) -> bool:\n",
        "        return True\n",
        "\n",
        "    @property\n",
        "    def requires_reference(self) -> bool:\n",
        "        return False\n",
        "\n",
        "    @property\n",
        "    def evaluation_name(self) -> str:\n",
        "        return \"scored_dopeness\"\n",
        "\n",
        "    def _evaluate_strings(\n",
        "        self,\n",
        "        prediction: str,\n",
        "        input: Optional[str] = None,\n",
        "        reference: Optional[str] = None,\n",
        "        **kwargs: Any\n",
        "    ) -> dict:\n",
        "        evaluator_result = self.eval_chain.invoke(\n",
        "            {\"input\": input, \"prediction\": prediction}, kwargs\n",
        "        )\n",
        "        reasoning, score = evaluator_result.content.split(\"\\n\", maxsplit=1)\n",
        "        score = re.search(r\"\\d+\", score).group(0)\n",
        "        if score is not None:\n",
        "            score = float(score.strip()) / 100.0\n",
        "        return {\"score\": score, \"reasoning\": reasoning.strip()}"
      ],
      "metadata": {
        "id": "qofRv8FI7TeZ"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 4: Initializing our evaluator config\n",
        "\n",
        "Now we can initialize our `RunEvalConfig` which we can use to evaluate our chain against our dataset.\n",
        "\n",
        "> NOTE: Check out the [documentation](https://docs.smith.langchain.com/evaluation/faq/custom-evaluators) for adding additional custom evaluators."
      ],
      "metadata": {
        "id": "-PoETszTMSNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.smith import RunEvalConfig, run_on_dataset\n",
        "\n",
        "eval_config = RunEvalConfig(\n",
        "    custom_evaluators=[DopenessEvaluator()],\n",
        "    evaluators=[\n",
        "        \"criteria\",\n",
        "        RunEvalConfig.Criteria(\"harmfulness\"),\n",
        "        RunEvalConfig.Criteria(\n",
        "            {\n",
        "                \"AI\": \"Does the response feel AI generated?\"\n",
        "                \"Response Y if they do, and N if they don't.\"\n",
        "            }\n",
        "        ),\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "pc0bedbe-S2z"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 5: Evaluating our RAG pipeline\n",
        "\n",
        "All that's left to do now is evaluate our pipeline!"
      ],
      "metadata": {
        "id": "8XalvsOjMvdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "client.run_on_dataset(\n",
        "    dataset_name=dataset_name,\n",
        "    llm_or_chain_factory=retrieval_augmented_qa_chain,\n",
        "    evaluation=eval_config,\n",
        "    verbose=True,\n",
        "    project_name=\"HF RAG Pipeline - Evaluation - v3\",\n",
        "    project_metadata={\"version\": \"1.0.0\"},\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6syFWlaF-olk",
        "outputId": "2c38f776-6dc5-4ce7-e6dc-6d42ae2cf934"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for project 'HF RAG Pipeline - Evaluation - v3' at:\n",
            "https://smith.langchain.com/o/ec0ab543-3d20-5b47-b548-e29646706b91/datasets/bc0ff1d4-4b34-4cfc-b0f2-7919cd259454/compare?selectedSessions=15de38a5-a589-4713-ac8a-1a071b739a16\n",
            "\n",
            "View all tests for Dataset QLoRA RAG Dataset at:\n",
            "https://smith.langchain.com/o/ec0ab543-3d20-5b47-b548-e29646706b91/datasets/bc0ff1d4-4b34-4cfc-b0f2-7919cd259454\n",
            "[------------------------------------------------->] 6/6\n",
            " Experiment Results:\n",
            "        feedback.helpfulness  feedback.harmfulness  feedback.AI  feedback.scored_dopeness error  execution_time                                run_id\n",
            "count                   6.00                  6.00         6.00                      6.00     0            6.00                                     6\n",
            "unique                   NaN                   NaN          NaN                       NaN     0             NaN                                     6\n",
            "top                      NaN                   NaN          NaN                       NaN   NaN             NaN  5c87e280-e994-4b5f-87ff-97fe8b6768b2\n",
            "freq                     NaN                   NaN          NaN                       NaN   NaN             NaN                                     1\n",
            "mean                    0.67                  0.00         0.17                      0.71   NaN            3.53                                   NaN\n",
            "std                     0.52                  0.00         0.41                      0.31   NaN            2.77                                   NaN\n",
            "min                     0.00                  0.00         0.00                      0.10   NaN            1.29                                   NaN\n",
            "25%                     0.25                  0.00         0.00                      0.74   NaN            1.94                                   NaN\n",
            "50%                     1.00                  0.00         0.00                      0.85   NaN            2.01                                   NaN\n",
            "75%                     1.00                  0.00         0.00                      0.85   NaN            4.92                                   NaN\n",
            "max                     1.00                  0.00         1.00                      0.90   NaN            8.08                                   NaN\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'project_name': 'HF RAG Pipeline - Evaluation - v3',\n",
              " 'results': {'086e5006-c51c-4bd9-a31a-2e969632db8e': {'input': {'question': 'What significant improvements does the LoRA system make?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nTo assess this, we need to consider whether the submission provides useful, insightful, and appropriate information in response to the input question. \\n\\nThe input question asks for the significant improvements that the LoRA system makes. \\n\\nThe submission provides a detailed response, outlining three key improvements that the LoRA system offers over traditional fine-tuning practices. These include efficiency, resource-efficient customization, and notable improvements in efficiency through an extension of LoRA. \\n\\nThe submission also provides a summary that reiterates the main points, further enhancing its helpfulness. \\n\\nTherefore, the submission is helpful, insightful, and appropriate, meeting the given criterion. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('384a8a0c-ff56-4c6b-a74d-19c69ef7b11c'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this assessment is whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a detailed explanation of the improvements that the LoRA system makes. The language used is professional and informative. There are no elements of harm, offense, or inappropriateness in the content.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('ae2fd08d-bd59-4ca5-9554-673e749bdeb2'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion asks if the response feels AI generated. The response is well-structured, coherent, and provides a detailed explanation of the improvements offered by the LoRA system. It uses technical language appropriately and maintains a consistent tone throughout. While AI has become quite advanced, the response does not exhibit any telltale signs of being AI-generated, such as awkward phrasing, lack of coherence, or inappropriate use of language. Therefore, it does not feel AI generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('c8a0e7f9-e404-46a5-b1a1-f28a18571293'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.9, value=None, comment='The response is very informative and detailed. It provides a clear and concise explanation of the improvements that the LoRA system makes. The response is structured well, with each point being clearly outlined and explained. The language used is professional and appropriate for the topic. The response also does a good job of summarizing the points at the end. However, it could be improved by providing more specific examples or data to support the points made.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('9a7eb40e-8bb4-4656-b84f-3d39bc79a537'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 8.077437,\n",
              "   'run_id': '5c87e280-e994-4b5f-87ff-97fe8b6768b2',\n",
              "   'output': '\\nAnswer:\\nThe LoRA system offers several significant improvements over traditional fine-tuning practices:\\n\\n1. Efficient alternative to full model retraining: LoRA reduces the computational cost of fine-tuning by applying low-rank matrices to modify pre-trained weights, making it a more efficient alternative to full model retraining.\\n2. Resource-efficient customization of LLMs: LoRA allows for customization of LLMs without requiring significant computational resources, making it a more practical solution for deploying LLMs in resource-constrained environments.\\n3. Notable improvements in efficiency: QLoRA, an extension of LoRA, has been shown to offer notable improvements in the efficiency of fine-tuning, further reducing the computational cost of customizing LLMs.\\n\\nIn summary, the LoRA system offers significant improvements in the efficiency of fine-tuning LLMs, making it a more practical and cost-effective solution for deploying LLMs in a variety of applications.'},\n",
              "  '38c4ed66-d994-40ab-83be-9435959ec7da': {'input': {'question': 'What is the most popular deep learning framework?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nThe submission provides a direct answer to the question asked in the input, which is about the most popular deep learning framework. The answer given is \"TensorFlow\", which is indeed a widely used and recognized deep learning framework. \\n\\nThe submission is helpful as it provides a clear and concise answer to the question. It is insightful as it provides information about a popular tool in the field of deep learning. The submission is also appropriate as it directly addresses the question and does not include any irrelevant or inappropriate content.\\n\\nTherefore, the submission meets the criterion of being helpful, insightful, and appropriate.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('4cddd84b-7e6c-4151-997c-06bae392eca9'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion is asking whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a simple factual statement about a deep learning framework. There is no harmful, offensive, or inappropriate content in the submission.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('d03e5133-dd7e-467d-afbc-e872e021a5b5'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion asks if the response feels like it was generated by an AI. The response provided is \"Based on the provided context, the most popular deep learning framework is TensorFlow.\" This response is clear, concise, and directly answers the question. It does not exhibit any of the common signs of AI-generated text, such as awkward phrasing, irrelevant information, or lack of coherence. Therefore, it does not feel AI generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('907cf922-dc92-4f0d-8859-dd13ccf127b2'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.85, value=None, comment='The response is accurate, clear, and concise. It directly answers the question without any unnecessary information or fluff. However, it lacks a bit of depth and detail that could make it more interesting or engaging. For example, it could have included a brief explanation of why TensorFlow is the most popular or what makes it stand out from other deep learning frameworks.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('c1119462-1801-4391-92b1-fe71f9ac6748'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 1.969239,\n",
              "   'run_id': '9e61dfb5-a1c1-4347-8245-71bd97ca720e',\n",
              "   'output': '\\nAnswer:\\nBased on the provided context, the most popular deep learning framework is TensorFlow.'},\n",
              "  'c1b7f0c1-7511-4642-a0b2-b66d1cb36263': {'input': {'question': 'What optimizer is used in QLoRA?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is \"helpfulness\". To assess this, we need to consider whether the submission is helpful, insightful, and appropriate.\\n\\n1. Helpfulness: The submission attempts to answer the question, but the information provided is incorrect. QLoRA, which stands for Quantum Long Range Architecture, uses a quantum optimizer, not \"Paged Optimizers\". Therefore, the submission is not helpful.\\n\\n2. Insightfulness: The submission does not provide any insightful information about the optimizer used in QLoRA. It simply states an incorrect fact.\\n\\n3. Appropriateness: While the submission is related to the topic of the question, it is not appropriate because it provides incorrect information.\\n\\nBased on these assessments, the submission does not meet the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('1082fed9-da59-49de-b6b7-35abdea92998'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion is asking whether the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is a simple factual statement about the optimizer used in QLoRA. There is no language or content that could be considered harmful, offensive, or inappropriate. \\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('1f4dad79-2a0f-4d44-99c7-d469e89f74a0'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is to assess whether the response feels AI generated or not. The response provided does not contain any obvious signs of being AI generated such as nonsensical phrases, lack of context, or robotic language. The response is coherent and seems to be based on some form of understanding, even if the information provided is incorrect. Therefore, the response does not feel AI generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('4f8eb119-766b-44e0-81d9-1111c3b9d3b1'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.7, value=None, comment='The response is accurate and informative, providing a direct answer to the question asked. However, it lacks a bit of flair or personality that could make it more engaging or \"dope\". It\\'s also assuming that the user knows what \"Paged Optimizers\" are, which may not always be the case. It could be improved by adding a brief explanation of what Paged Optimizers are or why they\\'re used in QLoRA.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('b4e56871-066c-4c29-9e07-ed76cd7569c9'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 2.056214,\n",
              "   'run_id': 'a2888d7f-d66b-4225-97c2-18bb890a29ea',\n",
              "   'output': '\\nAnswer:\\nAccording to the provided context, QLoRA uses Paged Optimizers as an optimizer.'},\n",
              "  '1fb0d976-3db4-4205-a6b2-0c180167fc9c': {'input': {'question': 'What data type was created in the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is \"helpfulness\". \\n\\nTo assess this, we need to consider whether the submission is helpful, insightful, and appropriate. \\n\\nThe submission provides a direct answer to the question asked, which is about the data type created in the QLoRA paper. The answer given is \"4-bit NormalFloat\". \\n\\nThis answer is helpful as it provides the exact information asked for in the question. \\n\\nThe answer is also appropriate as it directly relates to the question and does not include any irrelevant or inappropriate information. \\n\\nHowever, the criterion also asks if the submission is insightful. The submission does not provide any additional information or insight beyond the direct answer to the question. \\n\\nDespite this, the question does not seem to require any additional insight beyond the direct answer, so the lack of additional insight in the submission may not be a problem in this case. \\n\\nBased on this analysis, it seems that the submission does meet the criterion of being helpful.\\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('4c13ca21-a857-4eae-8bcc-161a8e10bf58'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion is asking if the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is a simple factual response to the question asked. It does not contain any harmful, offensive, or inappropriate content. \\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('cd93198e-cafc-42dc-b618-1576bfcc113c'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion is asking whether the response feels like it was generated by an AI. The response provided is straightforward and factual, providing a direct answer to the question asked. It does not exhibit any of the common signs of AI-generated text, such as nonsensical phrases, irrelevant information, or unusual sentence structures. Therefore, it does not feel AI generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('3513945c-2544-415d-a0c9-f4e3ae5d32ac'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.85, value=None, comment='The response is accurate, clear, and concise. It directly answers the question without any unnecessary information or fluff. It also uses professional language, which is appropriate for the context of the question. However, it could be improved by providing a bit more information about what the \"4-bit NormalFloat\" data type is or why it was created.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('ed70130f-ea06-477c-89b2-b00f1e8fa9f3'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 1.936737,\n",
              "   'run_id': 'eeec58b6-e923-46d4-84b8-8ecf6324640d',\n",
              "   'output': '\\nAnswer: \\nThe data type created in the QLoRA paper is called \"4-bit NormalFloat\".'},\n",
              "  '64334b32-c493-4d2f-83e2-bcdcc586e014': {'input': {'question': 'What is a Retrieval Augmented Generation system?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is the helpfulness of the submission. \\n\\nThe submission provides a clear and detailed explanation of what a Retrieval Augmented Generation (RAG) system is. It explains the concept by breaking it down into its components - retrieval and generation - and then describes how these components work together in a RAG system. \\n\\nThe submission also provides context by explaining how RAG systems are used in the task of language model quantization. This additional information can help the reader understand not just what a RAG system is, but also why it is useful. \\n\\nTherefore, the submission is helpful, insightful, and appropriate. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a637db6e-f09a-42cf-af74-fbd0172bd3e3'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion for this task is to assess whether the submission is harmful, offensive, or inappropriate.\\n\\nLooking at the submission, it is a detailed explanation of what a Retrieval Augmented Generation (RAG) system is. The language used is professional and informative. There is no use of offensive or inappropriate language. The content is relevant to the question asked and does not contain any harmful or misleading information.\\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('2db7be53-08f4-4285-aa73-1c81c1c89f54'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=0, value='N', comment='The criterion for this task is to assess whether the response feels like it was generated by an AI. \\n\\nLooking at the submission, the response is well-structured, coherent, and provides a detailed explanation of the concept of a Retrieval Augmented Generation system. It uses appropriate terminology and maintains a consistent tone throughout. \\n\\nWhile AI models can generate high-quality text, the response does not exhibit common signs of AI generation such as lack of coherence, abrupt changes in topic, or nonsensical sentences. \\n\\nTherefore, based on the given criterion, the response does not feel AI generated.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('a1a6b526-86a0-4a9b-b57b-84898fabf9ab'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.85, value=None, comment='This response is quite informative and detailed. It provides a clear and concise explanation of what a Retrieval Augmented Generation (RAG) system is, how it works, and its application in language model quantization. The language used is professional and appropriate for the context. However, it might be a bit too technical for someone without a background in this field, which could potentially make it less \"cool\" or \"dope\" to a general audience.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('4353cae4-5b56-4da4-a546-635b4f23f4fc'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 5.874389,\n",
              "   'run_id': '96c3c0c4-50c5-4ced-972c-09dfcf6f47e2',\n",
              "   'output': '\\nAnswer:\\nA Retrieval Augmented Generation (RAG) system is a type of language generation model that combines the strengths of both retrieval-based and generation-based models. The idea is to first retrieve relevant information from a large knowledge base using a retrieval model, and then use this retrieved information to augment the generation process. This can help improve the quality and relevance of the generated text by leveraging the knowledge and context from the retrieved information. In the context of the provided documents, RAG systems are discussed in relation to the task of language model (LLM) quantization, where the goal is to compress the LLM while preserving its ability to generate coherent and accurate text.'},\n",
              "  'e5104829-9e7e-442a-b592-41682d6541a3': {'input': {'question': 'Who authored the QLoRA paper?'},\n",
              "   'feedback': [EvaluationResult(key='helpfulness', score=0, value='N', comment='The criterion for this task is helpfulness. The submission should be helpful, insightful, and appropriate. \\n\\nLooking at the submission, the user has stated that they are unable to answer the question due to lack of context. This is an appropriate response given the circumstances. \\n\\nHowever, the response may not be considered helpful or insightful. The user has not provided any additional information or insight that could assist in finding the answer to the question. \\n\\nTherefore, based on the given criterion, the submission does not meet the criteria.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('860a7a06-ccfb-4cc4-99c3-a903923ef6ac'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='harmfulness', score=0, value='N', comment='The criterion asks if the submission is harmful, offensive, or inappropriate. \\n\\nLooking at the submission, it is a response to a question about the authorship of a paper. The respondent states that they are unable to answer the question due to lack of context. \\n\\nThere is nothing in the response that can be considered harmful, offensive, or inappropriate. The respondent is simply stating their inability to provide an answer due to insufficient information. \\n\\nTherefore, the submission is not harmful, offensive, or inappropriate.\\n\\nN', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('045e06e1-7a3a-4c81-8b51-c7f9223907bf'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='AI', score=1, value='Y', comment=\"The criterion asks if the response feels AI generated. The submission is a response stating that the AI is unable to answer the question due to lack of context. This is a typical response from an AI when it doesn't have enough information to generate a specific answer. Therefore, the submission does meet the criterion.\\n\\nY\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('b51d6b67-088a-4491-982a-5caa0e3233c9'))}, source_run_id=None, target_run_id=None),\n",
              "    EvaluationResult(key='scored_dopeness', score=0.1, value=None, comment='The response is not dope at all because it does not provide the information asked for. The user asked for the author of the QLoRA paper, and the AI was unable to provide that information. While it is true that the AI may not have access to all information, in this case, it is reasonable to expect that it could look up the author of a specific paper. The response is also not particularly engaging or interesting, which further lowers its \"dopeness\" score.', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('7890ec1e-9def-442b-b030-8eed1fbbddce'))}, source_run_id=None, target_run_id=None)],\n",
              "   'execution_time': 1.29224,\n",
              "   'run_id': '6a79becb-5651-435b-9170-2157884a7995',\n",
              "   'output': \"\\nAnswer:\\nI'm not able to answer that question as the context provided does not include the authorship of the QLoRA paper.\"}},\n",
              " 'aggregate_metrics': None}"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Nk93OrKjug2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}